{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram Segmentation\n",
    "\n",
    "In this example, we use [PyTorch](https://pytorch.org/) and [Lightning](https://lightning.ai/docs/pytorch/stable/) to train deep learning models to differentiate between 5G NR and 4G LTE signals within wideband spectrograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "**[Background](#Background):** Delve into the problem background and learn more about the machine learning frameworks, tools, and datasets used in this example.\n",
    "\n",
    "**[Set-up](#Set-Up):** Install the libraries and initialize the variables necessary to run the code in this notebook.\n",
    "\n",
    "**[Data Preprocessing](#Data-Preprocessing):** Load and analyze the Spectrum Sensing dataset.\n",
    "\n",
    "**[Model Training](#Model-Training):** Select and train a deep learning model.\n",
    "\n",
    "**[Model Verification](#Model-Verification):** Assess the performance of the model using a suite of common machine learning metrics\n",
    "\n",
    "**[Challenge Data](#Challange-Data):** Challenge the model on combined frames containing both LTE and NR signal.\n",
    "\n",
    "**[Conclusions & Next Step](#Conculsions-&-Next-Steps):** Interpret the results, summarize key learnings, and identify next steps for expanding upon this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "5G NR (New Radio) and 4G LTE (Long-Term Evolution) are both cellular network technologies, but they represent \n",
    "different generations of mobile network standards. The ability to identify and distinguish between the two holds significant \n",
    "applications in [spectrum sensing](https://iopscience.iop.org/article/10.1088/1742-6596/2261/1/012016) and serves as a foundational example showcasing the near-term feasibility of \n",
    "[intelligent radio](https://www.qoherent.ai/intelligentradio/) technology.\n",
    "\n",
    "A spectrogram, which depicts the frequency spectrum of a signal over time, is essentially just an image. Therefore, we can\n",
    "apply state-of-the-art [semantic segmentation](https://www.ibm.com/topics/semantic-segmentation) techniques from \n",
    "the field of computer vision to the problem of spectrogram analysis. Our task is to assign one of the \n",
    "following labels to each pixel in the spectrogram: 'LTE', 'NR', or 'Noise'. ('Noise' refers to the absence of signal, representing \n",
    "a vacant or empty spectrum, also known as whitespace.)\n",
    "\n",
    "The machine learning models utilized in this example are DeepLabV3 models. The DeepLabv3 framework was originally introduced by Chen _et al._ in their 2017 paper titled '[Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587)'. For an accessible introduction to the DeepLabV3 framework, please check out Isaac Berrios' article: [DeepLabv3: Building Blocks for Robust Segmentation Models](https://medium.com/@itberrios6/deeplabv3-c0c8c93d25a4).\n",
    "\n",
    "The dataset used in this example is the Spectrum Sensing dataset, provided by MathWorks. This dataset contains 900 LTE frames, 900 NR frames, and 900 combined frames with both LTE and NR signal. In this example, we train exclusively on the individual LTE and NR examples, excluding the combined frames.\n",
    "\n",
    "To ensure comparability with results obtained using MathWorks' AI-based network, we use the hyperparameter configuration from MathWorks' spectrum sensing example: [Spectrum Sensing with Deep Learning to Identify 5G and LTE Signals](https://www.mathworks.com/help/comm/ug/spectrum-sensing-with-deep-learning-to-identify-5g-and-lte-signals.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up\n",
    "\n",
    "In this section, we will install the dependencies required to run the code in this notebook. These dependencies include libraries and packages for tasks such as data manipulation, visualization, and machine learning. Additionally, we will initialize a few variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from osgeo import gdal\n",
    "from PIL import Image\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from typing import Optional, Any\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex as jac_ind\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassConfusionMatrix\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.v2 import Compose, PILToTensor, ToDtype, Normalize, ToPILImage, ToTensor\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50, deeplabv3_mobilenet_v3_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_font_size, label_font_size = 14, 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In semantic segmentation, the input data typically consists of images (in this case, spectrograms), while the output data consists of pixel-wise labels (masks) where each pixel is assigned a category label (in this case, either 'LTE', 'NR', or 'Noise'). \n",
    "\n",
    "In this example, we use [supervised learning](https://www.ibm.com/topics/supervised-learning) techniques to train our model. These techniques require both input spectrograms and the corresponding target masks for training. For each frame in the dataset, we have two separate files:\n",
    "\n",
    "- A `.png` file containing the spectrogram image to use as input to the model.\n",
    "\n",
    "- A `.hdf` ([HDF4](https://www.hdfgroup.org/solutions/hdf4/)) file containing the target mask to use for training.\n",
    "\n",
    "The Spectrum Sensing dataset also includes `.mat` files containing metadata such as the signal sample rate. However, none of this metadata is necessary for this example, so we can safely ignore these files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because spectrogram segmentation is a computer vision task, let's extend the [VisionDataset](https://pytorch.org/vision/main/generated/torchvision.datasets.VisionDataset.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrumSensing(VisionDataset):\n",
    "\n",
    "    def __init__(self, root: str, transform: Optional[callable] = None, target_transform: Optional[callable] = None):\n",
    "        \"\"\"Initialize the dataset, specifying the root directory where the dataset files are located.\"\"\"\n",
    "        super().__init__(root)\n",
    "\n",
    "        self.root = root\n",
    "        self.transform, self.target_transform = transform, target_transform\n",
    "\n",
    "        # Parse the root directory and extract the basenames of individual LTE and NR frames.\n",
    "        files = glob.glob(os.path.join(root, \"*.png\"))\n",
    "        self.frames = [os.path.basename(frame).split(\".\")[0] for frame in files]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[Image, Image]:\n",
    "        \"\"\"Return the image-mask pair at idx.\"\"\"\n",
    "        basename = self.frames[idx]\n",
    "\n",
    "        image_file = os.path.join(self.root, f\"{basename}.png\")\n",
    "        target_file = os.path.join(self.root, f\"{basename}.hdf\")\n",
    "\n",
    "        image = Image.open(image_file)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        mask_data = gdal.Open(target_file)\n",
    "        mask_band = mask_data.GetRasterBand(1)\n",
    "        mask = (Image.fromarray(mask_band.ReadAsArray())).convert(mode=\"L\")\n",
    "        if self.target_transform is not None:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice our `SpectrumSensing` class accepts two functions/transforms: `transform`, which is applied to the spectrogram, \n",
    "and `target_transform`, which is applied to the mask.\n",
    "\n",
    "Both the spectrograms and masks are 256 x 256 pixel images. However, the spectrograms are three channeled, while the masks are single-channeled. This is because the spectrograms are full RGB images, whereas the masks are ternary-valued images, where each pixel takes one of three discrete values:\n",
    "- `0`: Represents noise.\n",
    "- `127`: Representing 5G NR signal.\n",
    "- `255`: Representing 4G LTE signal.\n",
    "\n",
    "To prepare our spectrograms for training, we will convert them from PIL Images to Tensor objects. As required by our models, the images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`. To prepare our masks for training, we will convert them to Tensor objects, remove the extraneous channel dimension, and update the pixel values so that `0` respresents noise, `1` represents NR signal, and `2` represents LTE signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.getcwd()\n",
    "data_root = os.path.join(project_root, \"SpectrumSensingDataset\", \"TrainingData\")\n",
    "\n",
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "class Squeeze(torch.nn.Module):\n",
    "    def forward(self, target: Tensor):\n",
    "        return torch.squeeze(target)\n",
    "\n",
    "\n",
    "class DivideBy127(torch.nn.Module):\n",
    "    def forward(self, target: Tensor):\n",
    "        return torch.div(target, 127, rounding_mode=\"floor\")\n",
    "        \n",
    "transform = Compose(\n",
    "    [\n",
    "        PILToTensor(),\n",
    "        ToDtype(\n",
    "            torch.float, scale=True\n",
    "        ), \n",
    "        Normalize(mean=mean, std=std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_transform = Compose(\n",
    "    [\n",
    "        PILToTensor(), \n",
    "        Squeeze(), \n",
    "        DivideBy127(),  # Mapping 0 -> 0, 127 -> 1, and 255 -> 2.\n",
    "        ToDtype(torch.long)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that done, let's initialize the dataset, and take a closer look at a random training example and the corresponding mask. Due to our transforms, we expect that the image-mask pair will be returned as Tensor objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpectrumSensing(root=data_root, transform=transform, target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_index = np.random.randint(len(dataset))\n",
    "training_example, corresponding_mask = dataset[random_index]\n",
    "\n",
    "print(f\"The full dataset has {len(dataset)} examples. Loading example at index {random_index}:\")\n",
    "print(f\"Spectrogram: {type(training_example)}, {training_example.dtype}, {training_example.size()}\")\n",
    "print(f\"Mask: {type(corresponding_mask)}, {corresponding_mask.dtype}, {corresponding_mask.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset should contain 1,800 samples: 900 NR fames and 900 LTE frames. \n",
    "\n",
    "To get a better idea of what's going on, let's write some tranforms undo the previous normalization and prepare this image-mask pair for viewing. And, let's build a custom colormap for the masks, with noise as cyan, NR signal as blue, and LTE signal as purple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=[0.0, 0.0, 0.0], std=[1 / x for x in std]),\n",
    "        Normalize(mean=[-x for x in mean], std=[1.0, 1.0, 1.0]),\n",
    "        ToPILImage(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "inv_target_transform = Compose(\n",
    "    [\n",
    "        ToDtype(dtype=torch.uint8),\n",
    "        ToPILImage()\n",
    "    ]\n",
    ")\n",
    "\n",
    "training_example = inv_transform(training_example)\n",
    "corresponding_mask = inv_target_transform(corresponding_mask)\n",
    "\n",
    "values, labels, colors = [0, 1, 2], [\"Noise\", \"NR\", \"LTE\"], [\"cyan\", \"blue\", \"purple\"]\n",
    "mask_cmap = ListedColormap(colors)\n",
    "\n",
    "print(f\"Spectrogram: {training_example}\")\n",
    "print(f\"Mask: {corresponding_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(figsize=[8, 3.5], nrows=1, ncols=2)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "ax1.set_title(\"\\nRandom Spectrogram\", fontsize=title_font_size)\n",
    "ax2.set_title(\"Corresponding Mask\", fontsize=title_font_size)\n",
    "ax1.set_ylabel(\"Time [s]\", fontsize=label_font_size)\n",
    "ax2.set_ylabel(\"Time [s]\", fontsize=label_font_size)\n",
    "ax1.set_xlabel(\"Freq. [arb. units]\", fontsize=label_font_size)\n",
    "ax2.set_xlabel(\"Freq. [arb. units]\", fontsize=label_font_size)\n",
    "\n",
    "spect = ax1.imshow(training_example, vmin=0, vmax=255)\n",
    "fig.colorbar(spect, ax=ax1, fraction=0.04, ticks=[0, 255])\n",
    "\n",
    "mask = ax2.imshow(corresponding_mask, cmap=mask_cmap, vmin=0, vmax=2)\n",
    "mask_cbar = fig.colorbar(mask, ax=ax2, cmap=mask_cmap, fraction=0.04, ticks=[0.33, 1, 1.67])\n",
    "mask_cbar.ax.set_yticklabels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can view different examples from the dataset by rerunning the previous few code cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the relative frequencies of the different class labels. This step is critical for identifying imbalance in our dataset. Please note that the following code block might take a few seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = {label: 0 for label in labels}\n",
    "\n",
    "for _, mask in dataset:\n",
    "    arr = np.asarray(mask)\n",
    "    for i, label in enumerate(labels):\n",
    "        class_counts[label] += np.sum(arr == values[i])\n",
    "\n",
    "normalized_counts = np.array(list(class_counts.values())) / sum(list(class_counts.values()))\n",
    "\n",
    "plt.bar(class_counts.keys(), normalized_counts, tick_label=labels, color=colors)\n",
    "plt.title(\"Distribution of Pixel Counts by Class\", fontsize=title_font_size)\n",
    "plt.xlabel(\"Class\", fontsize=label_font_size)\n",
    "plt.ylabel(\"Counts (Normalized)\", fontsize=label_font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like most of our data is noise! A classification dataset like this—with skewed class proportions—is called imbalanced.\n",
    "\n",
    "An imbalanced dataset can result in biased and poorly performing models. Models trained on imbalanced data tends to focus more on the majority classes and may not learn enough about the minority classes. In our case, the majority class is noise, while the minority classes are the NR and LTE signals we want to identify and classify. To ensure the development of a fair, accurate, and robust model, we will need to address this class imbalance. \n",
    "\n",
    "But first, let's split the dataset into separate training and validation sets. The training dataset is the portion of the dataset that will be used to train the model, while the validation dataset will be held in reserve and used to evaluate the performance of the trained model. Let's start with a simple 80/20 split, where 80% of the dataset is used for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.80\n",
    "n_train_examples = int(len(dataset) * train_split)\n",
    "n_val_examples = len(dataset) - n_train_examples\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(\n",
    "    dataset=dataset, lengths=[n_train_examples, n_val_examples], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"The training split contains {len(train_set)} examples.\")\n",
    "print(f\"The validation split contains {len(val_set)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, data loaders facilitate easy access to samples, efficiently load and batch data, and offer numerous other features to streamline data preprocessing, management, and integration within the training loop. Let's create data loaders for both the training and validation datasets.\n",
    "\n",
    "In PyTorch, the `DataLoader` class allows us to pass a `batch_size` argument, which controls the number of samples used in each pass through the network. Using a small number of training examples each pass is called mini-batching, and can improve efficiency, stabilize training dynamics, and enable scalable training on large datasets. Choosing an appropriate mini-batch size depends on several factors, including the available memory on your hardware, training efficiency constraints, and generalization requirements. However, as with everything in machine learning, we ultimately rely on empirical testing to determine the optimal configuration that maximizes model performance for each specific task and dataset. In this example, we'll start with mini-batches containing 4 samples each, which will easily fit on any CPU/GPU without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=mini_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=mini_batch_size, shuffle=False)\n",
    "\n",
    "spects, masks = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch of spectrograms: {type(spects)}, {spects.dtype}, {spects.size()}\")\n",
    "print(f\"Batch of masks: {type(masks)}, {masks.dtype}, {masks.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a batch of spectrograms along with their corresponding masks. Note that the following plotting code is optimized for small batch sizes and may not render as nicely with larger batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spects = [inv_transform(i) for i in spects]\n",
    "masks = [inv_target_transform(i) for i in masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=[mini_batch_size * 2, 3], nrows=1, ncols=mini_batch_size, sharey=True)\n",
    "axes[0].set_ylabel(\"Time [s]\", fontsize=label_font_size)\n",
    "fig.text(0.5, 0.12, \"Freq. [arb. units]\", fontsize=label_font_size, ha=\"center\")\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_title(\"Spect \" + str(i + 1))\n",
    "    im = ax.imshow(spects[i], vmin=0, vmax=255)\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes(rect=[0.90, 0.25, 0.02, 0.5])\n",
    "fig.colorbar(im, cax=cbar_ax, ticks=[0, 255])\n",
    "\n",
    "fig, axes = plt.subplots(figsize=[mini_batch_size * 2, 3], nrows=1, ncols=mini_batch_size, sharey=True)\n",
    "axes[0].set_ylabel(\"Time [s]\", fontsize=label_font_size)\n",
    "fig.text(0.5, 0.12, \"Freq. [arb. units]\", fontsize=label_font_size, ha=\"center\")\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_title(\"Mask \" + str(i + 1))\n",
    "    im = ax.imshow(masks[i], vmin=0, vmax=2, cmap=mask_cmap)\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes(rect=[0.90, 0.25, 0.02, 0.5])\n",
    "cbar = fig.colorbar(im, cax=cbar_ax, ticks=[0.33, 1, 1.66])\n",
    "cbar.ax.set_yticklabels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Let's start by choosing a model. For this example, we suggest choosing between DeepLabV3 models with ResNet-50 or MobileNetV3 backbones. ResNet-50 is the deeper and more complex, and generally offers better model performance, whereas MobileNetV3 is designed to be lightweight and efficient. Because both models provide the same interface, and either will work with this example.\n",
    "\n",
    "Note: DeepLabV3 also provides a deeper ResNet-101 model. Feel free to experiment with it if you're interested, but we suggest 101 layers is  overkill for the task at hand and likely requires a larger dataset to train effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3  # We are dealing with three classes: Noise, NR, and LTE.\n",
    "\n",
    "# model = deeplabv3_resnet50(num_classes=n_classes)\n",
    "model = deeplabv3_mobilenet_v3_large(num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a loss function. A loss function, also known as a cost or objective function, measures how well a machine learning \n",
    "model's predictions match the actual target values. This quantifies the error between predicted outputs and ground truth labels, providing\n",
    "feedback that guides the model's training process. For classification problems, we commonly use the [Cross-Entropy Loss](https://machinelearningmastery.com/cross-entropy-for-machine-learning/), especially for \n",
    "multi-class classification problems. Let's use the [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) class from PyTorch, which allows us to assign different weights to individual classes during the computation of the loss. \n",
    "\n",
    "We'll use weights inversly propotional to the relative pixel count for each class. That way, we assign lower weights to overrepresented classes, like noise, and larger weights to underrepresented classes, like LTE signal. This reduces the impact of noise and allows the model to prioritize learning from NR and especially LTE samples. Class weighting is not the only way to address data imblance, but it is one of the more straightforward methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_count = statistics.median(list(class_counts.values()))\n",
    "weight = [median_count / class_counts[k] for k in class_counts.keys()]\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor(weight, dtype=torch.float))\n",
    "\n",
    "print(\"Class weights: \", {k: round(weight[i], 2) for i, k in enumerate(class_counts.keys())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will train out model using stochastic gradient descent (SGD). SGD is a variant of the standard [gradient descent](https://builtin.com/data-science/gradient-descent) optimizer where the loss function is computed on mini-batches of data rather than the entire dataset. This helps improve computational efficiency and scalability, particularly for large datasets, by updating model parameters based on the gradients computed on our mini-batches.\n",
    "\n",
    "We'll define the training and validation process of our segmentation model in a [`LightningModule`](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#lightningmodule). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationSGD(L.LightningModule):\n",
    "    \"\"\"LightningModule for training and evaluating a segmentation model using the SGD optimizer. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        loss_function: nn.Module,\n",
    "        n_classes: int,\n",
    "        learning_rate: float,\n",
    "        momentum: float,\n",
    "        weight_decay: float,\n",
    "        step_size: int,\n",
    "        gamma: float,\n",
    "    ):\n",
    "        \"\"\"Initializes the SegmentationSGD module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.train_accuracy = MulticlassAccuracy(num_classes=self.n_classes)\n",
    "        self.val_accuracy = MulticlassAccuracy(num_classes=self.n_classes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Defines a forward pass through the model.\"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        \"\"\"Defines a single training step.\"\"\"\n",
    "        image, target = batch\n",
    "        preds = self(image)['out']\n",
    "        loss = self.loss_function(preds, target)\n",
    "        self.train_accuracy(preds, target)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"Logs the training accuracy and loss metrics at the end of each training epoch.\"\"\"\n",
    "        self.log(name=\"train_accuracy\", value=self.train_accuracy, prog_bar=True)\n",
    "        self.log(name=\"train_loss\", value=loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        \"\"\"Defines a single validation step.\"\"\"\n",
    "        image, target = batch\n",
    "        preds = self(image)['out']\n",
    "        loss = self.loss_function(preds, target)\n",
    "        self.val_accuracy(preds, target)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Logs the training accuracy and loss metrics at the end of each validation epoch.\"\"\"\n",
    "        self.log(name=\"val_accuracy\", value=self.val_accuracy, prog_bar=True)\n",
    "        self.log(name=\"val_loss\", value=loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self) -> dict[str, Any]:\n",
    "        \"\"\"Configure the optimizer and learning rate scheduler.\"\"\"\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(), lr=self.learning_rate, momentum=self.momentum, weight_decay=self.weight_decay\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=self.step_size, gamma=self.gamma)\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer, \n",
    "            \"lr_scheduler\": lr_scheduler\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `SegmentationModelSGD` is initialized with several configuration settings that influence the behavior and performance of the machine learning algorithm or model. These parameters are called hyperparameters, and unlike model parameters, which are learned from the data during training, hyperparameters are set prior to training and influence the learning process.\n",
    "\n",
    "The following hyperparameters are used to configure the optimizer:\n",
    "- **Momentum:** A parameter that accelerates SGD in the relevant direction and dampens oscillations.\n",
    "- **Learning Rate:** The rate at which the model parameters are updated during optimization.\n",
    "- **Weight Decay:** A regularization term added to the loss function to penalize large weights in the model to prevent overfitting\n",
    "\n",
    "By gradually reducing the learning rate over epochs, the scheduler can help improve the convergence and stability of the optimization process\n",
    "We need to provide the following two parameters, which the learning rate scheduler uses to dynamically adjust the learning rate during training:\n",
    "- **Step Size:** The number of epochs after which the learning rate is reduced.\n",
    "- **Gamma:** The factor by which the learning rate is reduced after every step-size epochs.\n",
    "\n",
    "Adjusting these hyperparameters can significantly impact the training process and the final performance of the model, for better or for worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_module = SegmentationSGD(\n",
    "    model=model,\n",
    "    loss_function=loss_function,\n",
    "    n_classes=n_classes,\n",
    "    learning_rate=0.02,  # Represents the initial learning rate.\n",
    "    momentum=0.9,\n",
    "    weight_decay=1.0e-04,\n",
    "    step_size=10,\n",
    "    gamma=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: This is just a testing block, to be removed once the tutorial is complete\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "inputs, targets = next(iter(train_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Forward pass with the input data\n",
    "    preds = model(inputs)[\"out\"]\n",
    "\n",
    "print(\"Initial model predictions:\")\n",
    "print(preds.size())\n",
    "print(\"\\tPrediction max: \", torch.max(preds))\n",
    "print(\"\\tPrediction min: \", torch.min(preds))\n",
    "\n",
    "print(\"\\tMax value in target: \", torch.max(targets.long()))\n",
    "print(\"\\tMin value in target: \",torch.min(targets.long()))\n",
    "loss = loss_function(preds, targets.long())\n",
    "print(\"Loss: \", loss)\n",
    "\n",
    "# Convert preds to Image for viewing\n",
    "print(\"After converting back to images for viewing:\")\n",
    "print(\"Predictions: \", preds.size())\n",
    "\n",
    "# Need to find the classes with the largest probability.\n",
    "# Take the maximum value along the class axis.\n",
    "output = preds.argmax(1)\n",
    "\n",
    "print(\"Output: \", output.size())\n",
    "\n",
    "fig, axes = plt.subplots(figsize=[mini_batch_size * 2, 3], nrows=1, ncols=mini_batch_size, sharey=True)\n",
    "axes[0].set_ylabel(\"Time [s]\", fontsize=label_font_size)\n",
    "fig.text(0.5, 0.12, \"Freq. [arb. units]\", fontsize=label_font_size, ha=\"center\")\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_title(\"Prediction \" + str(i + 1))\n",
    "    im = ax.imshow(output[i], vmin=0, vmax=2, cmap=mask_cmap)\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes(rect=[0.90, 0.25, 0.02, 0.5])\n",
    "cbar = fig.colorbar(im, cax=cbar_ax, ticks=[0.33, 1, 1.66])\n",
    "cbar.ax.set_yticklabels(labels)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=[mini_batch_size * 2, 3], nrows=1, ncols=mini_batch_size, sharey=True)\n",
    "axes[0].set_ylabel(\"Time [s]\", fontsize=label_font_size)\n",
    "fig.text(0.5, 0.12, \"Freq. [arb. units]\", fontsize=label_font_size, ha=\"center\")\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_title(\"Mask \" + str(i + 1))\n",
    "    im = ax.imshow(targets[i], vmin=0, vmax=2, cmap=mask_cmap)\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes(rect=[0.90, 0.25, 0.02, 0.5])\n",
    "cbar = fig.colorbar(im, cax=cbar_ax, ticks=[0.33, 1, 1.66])\n",
    "cbar.ax.set_yticklabels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model, weighted loss function, and Lightning Module, we are prepared to train our model. If available, we will leverage GPU acceleration for training. Otherwise, the training process will default to using the CPU. Please be patient; model training time may vary depending on the current hardware configuration and could take a few minutes.\n",
    "\n",
    "The number of epochs determines how many times the entire dataset will be used to train the model. We will begin training with 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Training model on GPU.\")\n",
    "    trainer = L.Trainer(accelerator=\"gpu\", max_epochs=n_epochs, logger=True)\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"Training model on CPU.\")\n",
    "    trainer = L.Trainer(max_epochs=n_epochs, logger=True)\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(len(train_loader))\n",
    "trainer.fit(model=segmentation_module, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Verification\n",
    "\n",
    "Having trained our model, the next step is to evaluate its performance. To accomplish this, we'll use a suite of standard machine learning metrics. But first, let's take quick look at a random batch of predictions and true labels.\n",
    "\n",
    "Because the model returns the unnormalized probabilities corresponding to the predictions of each class. We need to use `argmax()` to get the maximum prediction of each class. The result is a ternary-valued image for each example in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "spects, masks = next(iter(train_loader))\n",
    "spects = spects.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = (model(spects)[\"out\"]).argmax(1)\n",
    "\n",
    "preds, spects = preds.cpu(), [inv_transform(i.cpu()) for i in spects]\n",
    "print(\"Predictions:\", preds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=[mini_batch_size * 2, 3], nrows=1, ncols=mini_batch_size, sharey=True)\n",
    "axes[0].set_ylabel(\"Time [s]\", fontsize=label_font_size)\n",
    "fig.text(0.5, 0.12, \"Freq. [arb. units]\", fontsize=label_font_size, ha=\"center\")\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_title(\"Spect \" + str(i + 1))\n",
    "    im = ax.imshow(spects[i], vmin=0, vmax=255)\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes(rect=[0.90, 0.25, 0.02, 0.5])\n",
    "fig.colorbar(im, cax=cbar_ax, ticks=[0, 255])\n",
    "\n",
    "fig, axes = plt.subplots(figsize=[mini_batch_size * 2, 3], nrows=1, ncols=mini_batch_size, sharey=True)\n",
    "axes[0].set_ylabel(\"Time [s]\", fontsize=label_font_size)\n",
    "fig.text(0.5, 0.12, \"Freq. [arb. units]\", fontsize=label_font_size, ha=\"center\")\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_title(\"Mask \" + str(i + 1))\n",
    "    im = ax.imshow(masks[i], vmin=0, vmax=2, cmap=mask_cmap)\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes(rect=[0.90, 0.25, 0.02, 0.5])\n",
    "cbar = fig.colorbar(im, cax=cbar_ax, ticks=[0.33, 1, 1.66])\n",
    "cbar.ax.set_yticklabels(labels)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=[mini_batch_size * 2, 3], nrows=1, ncols=mini_batch_size, sharey=True)\n",
    "axes[0].set_ylabel(\"Time [s]\", fontsize=label_font_size)\n",
    "fig.text(0.5, 0.12, \"Freq. [arb. units]\", fontsize=label_font_size, ha=\"center\")\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_title(\"Prediction \" + str(i + 1))\n",
    "    im = ax.imshow(preds[i], vmin=0, vmax=2, cmap=mask_cmap)\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes(rect=[0.90, 0.25, 0.02, 0.5])\n",
    "cbar = fig.colorbar(im, cax=cbar_ax, ticks=[0.33, 1, 1.66])\n",
    "cbar.ax.set_yticklabels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the confusion matrix, which provides a comprehensive overview of the model's ability. The diagonal elements represent the correct predictions and off-diagonal elements indicate prediction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(\n",
    "    model: nn.Module, val_loader: DataLoader, n_classes: int, device: str, normalize: Optional[str] = \"true\"\n",
    ") -> MulticlassConfusionMatrix:\n",
    "    \"\"\"Compute the confusion matrix for a given model using the validation dataset.\"\"\"\n",
    "    conf_matrix = (MulticlassConfusionMatrix(num_classes=num_classes, normalize=normalize)).to(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = (model(x)[\"out\"]).argmax(dim=1)\n",
    "            conf_matrix.update(pred, y)\n",
    "\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(model=segmentation_module, val_loader=val_loader, n_classes=n_classes, device=device)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(3, 3))\n",
    "ax.set_title(\"Confusion Matrix\", fontsize=title_font_size)\n",
    "ax.set_xlabel(\"True label\", fontsize=label_font_size)\n",
    "ax.set_ylabel(\"Predicted label\", fontsize=label_font_size)\n",
    "\n",
    "displ = ConfusionMatrixDisplay(np.array(torch.round(conf_matrix.compute(), decimals=2).to(\"cpu\")))\n",
    "displ.plot(ax=ax, colorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's examine the model accuracy, calculated as the ratio of correctly predicted pixels to the total number of pixels. However, it's important to note that accuracy alone doesn't tell the whole story. Due to the imbalance in our dataset, a high accuracy can be achieved by always predicting  noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_module.eval()\n",
    "scores = trainer.validate(segmentation_module, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Histogram of Intersection over Union (IoU) Scores per Image_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a more comprehensive report, complete with the following metrics:\n",
    "\n",
    "- Recall: The recall (sensitivity) measures the ability of the model to identify all relevant pixels.\n",
    "\n",
    "- Precision: The precision assesses the accuracy of positive predictions.\n",
    "\n",
    "- F1 Score: The F1 score combines both recall and precision into a single value, providing a more balanced measure of the model's performance.\n",
    "\n",
    "- Intersection over Union (IoU): The IoU quantifies the overlap between the predicted bounding box or segmented region and the ground truth bounding box or annotated region from a dataset. A higher IoU value indicates a better alignment between the predicted and actual regions, reflecting a more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(model, loader, metric_name, num_classes, device):\n",
    "    metric = getattr(torchmetrics.classification, metric_name)\n",
    "    metric_per_class = metric(num_classes=num_classes, average=\"none\").to(device)\n",
    "    average_metric = metric(num_classes=num_classes, average=\"macro\").to(device)\n",
    "    weighted_metric = metric(num_classes=num_classes, average=\"weighted\").to(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = (model(x)[\"out\"]).argmax(dim=1)\n",
    "            pred = pred.to(device)\n",
    "            metric_per_class.update(pred, y)\n",
    "            average_metric.update(pred, y)\n",
    "            weighted_metric.update(pred, y)\n",
    "\n",
    "    value_per_class = metric_per_class.compute()\n",
    "    average_value = average_metric.compute()\n",
    "    weighted_value = weighted_metric.compute()\n",
    "    return value_per_class, average_value.unsqueeze(0), weighted_value.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"MulticlassRecall\", \"MulticlassPrecision\", \"MulticlassF1Score\", \"MulticlassJaccardIndex\"]\n",
    "\n",
    "metric_results = {\n",
    "    metric_name: torch.hstack(compute_metric(segm_sgd_model, val_loader, metric_name, NUM_CLASSES, device))\n",
    "    for metric_name in metric_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_results_cpu = {key: (value.to(\"cpu\")).numpy() for key, value in metric_results.items()}\n",
    "\n",
    "classification_report = pd.DataFrame(metric_results_cpu)\n",
    "\n",
    "index = [\"Noise\", \"NR\", \"LTE\", \"macro avg\", \"weighted avg\"]\n",
    "columns = [\"Recall\", \"Precision\", \"F1 Score\", \"IoU\"]\n",
    "\n",
    "classification_report.columns = columns\n",
    "classification_report.index = index\n",
    "\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at a histogram of IoU Scores per Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(model, loader, device):\n",
    "    iou_scores = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = (model(x)[\"out\"]).argmax(dim=1)\n",
    "            jaccard = jac_ind(task=\"multiclass\", num_classes=3).to(device)\n",
    "            score = jaccard(pred.to(device), y)\n",
    "            iou_scores.append(score.item())\n",
    "\n",
    "    plt.hist(iou_scores, color=\"green\", histtype=\"bar\")\n",
    "    plt.xlabel(\"IoU\", color=\"blue\")\n",
    "    plt.ylabel(\"Number of Masks\", color=\"blue\")\n",
    "    plt.title(\"Mean IoU\", color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_val_loader = DataLoader(val_subset, batch_size=1, shuffle=False)  # Changed the batch_size\n",
    "plot_hist(segmentation_module, new_val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Identification in Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_label(mask) -> str:\n",
    "    \"\"\"\n",
    "    :param mask: The mask image containing signal labels.\n",
    "\n",
    "    :return: The signal label based on unique labels in the mask.\n",
    "    \"\"\"\n",
    "    labels = {0: \"Noise\", 1: \"NR\", 2: \"LTE\"}\n",
    "    unique_labels_in_mask = torch.unique(mask)\n",
    "\n",
    "    if len(unique_labels_in_mask) == 2:\n",
    "        key = torch.unique(mask)[1].item()\n",
    "        return labels[key]\n",
    "\n",
    "    elif len(unique_labels_in_mask) == 3:\n",
    "        key_1 = torch.unique(mask)[1].item()\n",
    "        key_2 = torch.unique(mask)[2].item()\n",
    "        return labels[key_1] + \"_\" + labels[key_2]\n",
    "\n",
    "    else:\n",
    "        key = torch.unique(mask)[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram_mask(image, mask: Image, target: Image = None):\n",
    "\n",
    "    if target is not None:\n",
    "        fig, ax = plt.subplots(3, 1, figsize=(4, 10))\n",
    "\n",
    "        ax[0].imshow(torch.permute(image, (1, 2, 0)))\n",
    "        ax[0].set_xlabel(\"Frequency\", fontsize=12, color=\"blue\")\n",
    "        ax[0].set_ylabel(\"Time\", fontsize=12, color=\"blue\")\n",
    "        ax[0].set_title(f\"Received Spectrogram ({signal_label(target)})\", color=\"blue\")\n",
    "\n",
    "        ax[1].set_ylabel(\"Time\", fontsize=12, color=\"blue\")\n",
    "        ax[1].set_xlabel(\"Frequency\", fontsize=12, color=\"blue\")\n",
    "        ax[1].imshow(target)\n",
    "        ax[1].set_title(f\"True Signal Label ({signal_label(target)})\", color=\"blue\")\n",
    "\n",
    "        ax[2].imshow(mask)\n",
    "        # plt.imshow(predicted_image.permute(1,2,0)[:,:,0])\n",
    "        # or equivalently\n",
    "        # plt.imshow(pred['out'][0][0].to('cpu').detach())\n",
    "        ax[2].set_xlabel(\"Frequency\", fontsize=12, color=\"blue\")\n",
    "        ax[2].set_ylabel(\"Time\", fontsize=12, color=\"blue\")\n",
    "        ax[2].set_title(f\"Prediction ({signal_label(mask)})\", color=\"blue\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "    else:\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(4, 10))\n",
    "\n",
    "        ax[0].imshow(torch.permute(image, (1, 2, 0)))\n",
    "        ax[0].set_xlabel(\"Frequency\", fontsize=12, color=\"blue\")\n",
    "        ax[0].set_ylabel(\"Time\", fontsize=12, color=\"blue\")\n",
    "        ax[0].set_title(\"Received Spectrogram\", color=\"blue\")\n",
    "\n",
    "        ax[1].imshow(mask)\n",
    "        # plt.imshow(predicted_image.permute(1,2,0)[:,:,0])\n",
    "        # or equivalently\n",
    "        # plt.imshow(pred['out'][0][0].to('cpu').detach())\n",
    "        ax[1].set_xlabel(\"Frequency\", fontsize=12, color=\"blue\")\n",
    "        ax[1].set_ylabel(\"Time\", fontsize=12, color=\"blue\")\n",
    "        ax[1].set_title(f\"Prediction ({signal_label(mask)})\", color=\"blue\")\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target = next(iter(val_loader))  # First batch of spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target = image.to(device), target.to(device)\n",
    "segm_sgd_model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_masks = segm_sgd_model(image)[\"out\"]\n",
    "    first_mask_in_batch = predicted_masks[0].argmax(dim=0)\n",
    "\n",
    "first_image_in_batch = image[0]\n",
    "first_target_in_batch = target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram_mask(first_image_in_batch.to(\"cpu\"), first_mask_in_batch.to(\"cpu\"), first_target_in_batch.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Data\n",
    "\n",
    "TODO: Testing Model with Combined (NR_LTE) Signals\n",
    "\n",
    "Recall combined frames with both NR and LTE signals were excluded from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the first NR_LTE signal\n",
    "PATH_TO_MATLAB5G_TRAINING_DATA = os.getcwd()\n",
    "spec_path = os.path.join(PATH_TO_MATLAB5G_TRAINING_DATA, \"LTE_NR\", \"LTE_NR_frame_0.png\")\n",
    "spectrogram = read_image(spec_path)  # Image has both NR and LTE signal\n",
    "spectrogram = spectrogram.to(device)\n",
    "\n",
    "segm_sgd_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = segm_sgd_model((spectrogram.to(torch.float)).unsqueeze(0))[\"out\"]\n",
    "    mask = pred[0].argmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram_mask(spectrogram.to(\"cpu\"), mask.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions & Next Steps\n",
    "\n",
    "In this example, we used PyTorch and PyTorch Lightning to train DeepLabV3 models to identify and differentiate between 5G NR and 4G LTE signals within wideband spectrograms, showcasing one of the ways we can leverage machine learning to identify things in the wireless spectrum. This involved data analysis and preprocessing, model selection, choosing a loss function and optimizer, model training, model performance validation, and finally testing the model's generalization on combined frames containing both NR and LTE signals, \n",
    "\n",
    "The capability to differentiate and recognize various signals finds direct applications in spectrum sensing, which is fundamental to autonomous spectrum management, and brings us one step closer to more holistic cognitive radio solutions! 📡🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope this example was informative. Here are some next steps you can take to further explore and expand upon what you've learned:\n",
    "\n",
    "- **Experiment with the Hyperparameters:** Adjust the values of hyperparameters such as the number of training epochs, mini-batch size, and learning rate, and observe how these configurations influence model training and performance. After gaining insights through manual hyperparameter tuning, explore automated approaches using tools like [Ray Tune](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) or [Optuna](https://optuna.org/).\n",
    "\n",
    "- **Explore Alternative Solutions to Class Imbalance:** In this example, we addressed class imbalance in our dataset using a weighted cross-entropy loss function. Research and implement alternative strategies or loss functions designed to address imbalance in image datasets.\n",
    "\n",
    "- **Integrate Combined Frames:** In this example, we trained exclusively on the individual NR and LTE frames. Try integrating the combined frames that contain both the NR and LTE signals into the training process, and evaluate the effect on model performance and generalization.\n",
    "\n",
    "- **Test your Model on Captured Radio Data:** If you have radio hardware available, consider testing your model on real recordings of live radio data. Check out this article from MathWorks for more information on how to capture 5G NR and LTE signals: [Capture and Label NR and LTE Signals for AI Training](https://www.mathworks.com/help/wireless-testbench/ug/capture-and-label-nr-and-lte-signals-for-ai-training.html).\n",
    "\n",
    "- **Explore RIA Core on GitHub:** At Qoherent, we're building [Radio Intelligence Applications](https://qoherent.ai/radiointelligenceapps-project/) (RIA) to drive the creation of intelligent radios. Check out [RIA Core](https://github.com/qoherent/ria)—the free and open-source foundation of RIA—and consider contributing to the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
