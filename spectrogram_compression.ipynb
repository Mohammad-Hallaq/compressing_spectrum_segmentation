{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram Segmentation\n",
    "\n",
    "In this example, we use [PyTorch](https://pytorch.org/) and [Lightning](https://lightning.ai/docs/pytorch/stable/) to train a deep learning model to identify and differentiate between 5G NR and 4G LTE signals within wideband spectrograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "**[Background](#Background):** Delve into the problem background and learn more about the machine learning frameworks, tools, and datasets used in this example.\n",
    "\n",
    "**[Set-up](#Set-Up):** Install the libraries necessary to run the code in this notebook.\n",
    "\n",
    "**[Data Preprocessing](#Data-Preprocessing):** Load and analyze the Spectrum Sensing dataset.\n",
    "\n",
    "**[Model Compression](#Model-Compression):** Configure, train, and compress a Deeplabv3 model with a MobileNetV3 backbone.\n",
    "\n",
    "**[Model Validation](#Model-Validation):** Assess the performance of the model after compression using a suite of common machine learning metrics.\n",
    "\n",
    "**[Challenge Data](#Challenge-Data):** Challenge the model on combined frames containing both LTE and NR signal.\n",
    "\n",
    "<!-- **[Conclusions & Next Steps](#Conclusions-&-Next-Steps):** Interpret the results, summarize key learnings, and identify steps for expanding upon this example. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "5G NR (New Radio) and 4G LTE (Long-Term Evolution) are both cellular network technologies, but they represent \n",
    "different generations of mobile network standards. The ability to identify and distinguish between the two holds significant \n",
    "applications in [spectrum sensing](https://iopscience.iop.org/article/10.1088/1742-6596/2261/1/012016) and serves as a foundational example showcasing the near-term feasibility of \n",
    "[intelligent radio](https://www.qoherent.ai/intelligentradio/) technology.\n",
    "\n",
    "A spectrogram, which depicts the frequency spectrum of a signal over time, is essentially just an image. Therefore, we can\n",
    "apply state-of-the-art [semantic segmentation](https://www.ibm.com/topics/semantic-segmentation) techniques from \n",
    "the field of computer vision to the problem of spectrogram analysis. Our task is to assign one of the \n",
    "following labels to each pixel in the spectrogram: 'LTE', 'NR', or 'Noise'. ('Noise' refers to the absence of signal, representing \n",
    "a vacant or empty spectrum, also known as whitespace.)\n",
    "\n",
    "The machine learning model utilized in this example is a DeepLabV3 model with a MobileNetV3 large backbone. The DeepLabv3 framework was originally introduced by Chen _et al._ in their 2017 paper titled '[Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587) and the MobileNetV3 backbone was developed by Howard _et al._ and is further discussed in their 2019 paper titled '[Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)'. For an accessible introduction to the DeepLabV3 framework, please check out Isaac Berrios' article: [DeepLabv3: Building Blocks for Robust Segmentation Models](https://medium.com/@itberrios6/deeplabv3-c0c8c93d25a4).\n",
    "\n",
    "The dataset used in this example is the Spectrum Sensing dataset, provided by MathWorks. This dataset contains 900 LTE frames, 900 NR frames, and 900 combined frames with both LTE and NR signal. In this example, we train exclusively on the individual LTE and NR examples, excluding the combined frames from the training process.\n",
    "\n",
    "To address real-world deployment challenges, such as resource-constrained environments in IoT devices and edge applications, the model is further compressed. The task involves reducing the model size while maintaining high performance. This step is essential as smaller models offer several advantages: they require less memory, enable faster inference, and consume lower power, making them ideal for deployment on devices with limited computational capabilities. By applying model compression techniques, including pruning and knowledge distillation, we optimize the DeepLabV3 model to ensure it is efficient and suitable for practical use in spectrum segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up\n",
    "\n",
    "In this section, we will install the dependencies required to run the code in this notebook. These dependencies include libraries and packages for tasks such as data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [Torch-Pruning (TP)](https://github.com/VainF/Torch-Pruning) framework to apply structural pruning to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch_pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import statistics\n",
    "from typing import Any, Optional\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib.colors import ListedColormap\n",
    "from osgeo import gdal\n",
    "from PIL import Image\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassConfusionMatrix,\n",
    "    MulticlassF1Score,\n",
    "    MulticlassJaccardIndex,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    ")\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.models.segmentation import (  \n",
    "    deeplabv3_mobilenet_v3_large,\n",
    "    deeplabv3_resnet50,\n",
    "    deeplabv3_resnet101,\n",
    ")\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    PILToTensor,\n",
    "    ToDtype,\n",
    "    ToPILImage,\n",
    ")\n",
    "\n",
    "import torch_pruning as tp\n",
    "import copy\n",
    "import math\n",
    "from torch.nn.functional import softmax, log_softmax, kl_div\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_font_size, label_font_size = 14, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)                \n",
    "torch.cuda.manual_seed(seed)          \n",
    "random.seed(seed)                     \n",
    "np.random.seed(seed)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In semantic segmentation, the input data typically consists of images (in this case, spectrograms), while the output data consists of pixel-wise labels (masks) where each pixel is assigned a category label (in this case, either 'LTE', 'NR', or 'Noise'). \n",
    "\n",
    "We will use [supervised learning](https://www.ibm.com/topics/supervised-learning) techniques to train our model. These techniques require both input spectrograms and the corresponding target masks for training. For each frame in the dataset, we have two separate files:\n",
    "\n",
    "- A `.png` file containing the spectrogram image to use as input to the model.\n",
    "\n",
    "- A `.hdf` ([HDF4](https://www.hdfgroup.org/solutions/hdf4/)) file containing the target mask to use for training.\n",
    "\n",
    "The Spectrum Sensing dataset also includes `.mat` files containing metadata such as the signal sample rate. However, none of this metadata is necessary for this example, so we can safely ignore these files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because spectrogram segmentation is a computer vision task, let's extend the [VisionDataset](https://pytorch.org/vision/main/generated/torchvision.datasets.VisionDataset.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrumSensing(VisionDataset):\n",
    "\n",
    "    def __init__(self, root: str, transform: Optional[callable] = None, target_transform: Optional[callable] = None):\n",
    "        \"\"\"Initialize the dataset, specifying the root directory where the dataset files are located.\"\"\"\n",
    "        super().__init__(root)\n",
    "\n",
    "        self.root = root\n",
    "        self.transform, self.target_transform = transform, target_transform\n",
    "\n",
    "        # Parse the root directory and extract the basenames of individual LTE and NR frames.\n",
    "        files = glob.glob(os.path.join(root, \"*.png\"))\n",
    "        self.frames = [os.path.basename(frame).split(\".\")[0] for frame in files]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[Image, Image]:\n",
    "        \"\"\"Return the image-mask pair at idx.\"\"\"\n",
    "        basename = self.frames[idx]\n",
    "\n",
    "        image_file = os.path.join(self.root, f\"{basename}.png\")\n",
    "        target_file = os.path.join(self.root, f\"{basename}.hdf\")\n",
    "\n",
    "        image = Image.open(image_file)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        mask_data = gdal.Open(target_file)\n",
    "        mask_band = mask_data.GetRasterBand(1)\n",
    "        mask = (Image.fromarray(mask_band.ReadAsArray())).convert(mode=\"L\")\n",
    "        if self.target_transform is not None:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice our `SpectrumSensing` class accepts two functions/transforms: `transform`, which is applied to the spectrogram, \n",
    "and `target_transform`, which is applied to the mask.\n",
    "\n",
    "Both the spectrograms and masks are 256 x 256 pixel images. However, the spectrograms are three channeled, while the masks are single-channeled. This is because the spectrograms are full RGB images, whereas the masks are ternary-valued images, where each pixel takes one of three discrete values:\n",
    "- `0`: Representing noise.\n",
    "- `127`: Representing NR signal.\n",
    "- `255`: Representing LTE signal.\n",
    "\n",
    "To prepare our spectrograms for training, we will convert them from PIL Images to Tensor objects. As required by our model, the images have to be loaded into the range `[0, 1]` and then normalized using a mean of `[0.485, 0.456, 0.406]` and a standard deviation of `[0.229, 0.224, 0.225]`. To prepare our masks for training, we will convert them to Tensor objects, remove the extraneous channel dimension, and update the pixel values such that `0` represents noise, `1` represents NR signal, and `2` represents LTE signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.getcwd()\n",
    "data_root = os.path.join(project_root, \"SpectrumSensingDataset\", \"TrainingData\")\n",
    "\n",
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "class Squeeze(torch.nn.Module):\n",
    "    def forward(self, target: Tensor):\n",
    "        return torch.squeeze(target)\n",
    "\n",
    "\n",
    "class DivideBy127(torch.nn.Module):\n",
    "    # Mapping 0 -> 0, 127 -> 1, and 255 -> 2.\n",
    "    def forward(self, target: Tensor):\n",
    "        return torch.div(target, 127, rounding_mode=\"floor\")\n",
    "\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        PILToTensor(),\n",
    "        ToDtype(torch.float, scale=True),\n",
    "        Normalize(mean=mean, std=std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_transform = Compose([PILToTensor(), Squeeze(), DivideBy127(), ToDtype(torch.long)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Now let's initialize the dataset, and take a closer look at a random training example and the corresponding mask. Due to our transforms, we expect that the image-mask pair will be returned as Tensor objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpectrumSensing(root=data_root, transform=transform, target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_index = np.random.randint(len(dataset))\n",
    "training_example, corresponding_mask = dataset[random_index]\n",
    "\n",
    "print(f\"The full dataset has {len(dataset)} examples. Loading example at index {random_index}.\")\n",
    "print(f\"Spectrogram: {type(training_example)}, {training_example.dtype}, {training_example.size()}\")\n",
    "print(f\"Mask: {type(corresponding_mask)}, {corresponding_mask.dtype}, {corresponding_mask.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset should contain 1,800 samples: 900 NR frames and 900 LTE frames. \n",
    "\n",
    "To gain further insight, let's write some transforms to undo the previous normalization and prepare this image-mask pair for viewing. And, let's build a custom colormap for the masks, with noise as cyan, NR signal as blue, and LTE signal as purple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=[0.0, 0.0, 0.0], std=[1 / x for x in std]),\n",
    "        Normalize(mean=[-x for x in mean], std=[1.0, 1.0, 1.0]),\n",
    "        ToPILImage(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "inv_target_transform = Compose([ToDtype(dtype=torch.uint8), ToPILImage()])\n",
    "\n",
    "training_example = inv_transform(training_example)\n",
    "corresponding_mask = inv_target_transform(corresponding_mask)\n",
    "\n",
    "values, labels, colors = [0, 1, 2], [\"Noise\", \"NR\", \"LTE\"], [\"cyan\", \"blue\", \"purple\"]\n",
    "mask_cmap = ListedColormap(colors)\n",
    "\n",
    "print(f\"Spectrogram: {training_example}\")\n",
    "print(f\"Mask: {corresponding_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(figsize=[8, 3.5], nrows=1, ncols=2)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "ax1.set_title(\"\\nRandom Spectrogram\", fontsize=title_font_size)\n",
    "ax2.set_title(\"Corresponding Mask\", fontsize=title_font_size)\n",
    "ax1.set_ylabel(\"Time [arb. units]\", fontsize=label_font_size)\n",
    "ax2.set_ylabel(\"Time [arb. units]\", fontsize=label_font_size)\n",
    "ax1.set_xlabel(\"Freq. [arb. units]\", fontsize=label_font_size)\n",
    "ax2.set_xlabel(\"Freq. [arb. units]\", fontsize=label_font_size)\n",
    "\n",
    "spect = ax1.imshow(training_example, vmin=0, vmax=255)\n",
    "fig.colorbar(spect, ax=ax1, fraction=0.045, ticks=[0, 255])\n",
    "\n",
    "mask = ax2.imshow(corresponding_mask, cmap=mask_cmap, vmin=0, vmax=2)\n",
    "mask_cbar = fig.colorbar(mask, ax=ax2, cmap=mask_cmap, fraction=0.045, ticks=[0.33, 1, 1.67])\n",
    "mask_cbar.ax.set_yticklabels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can view different examples from the dataset by rerunning the previous few code cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the relative frequencies of the different class labels. This step is critical for identifying imbalance in our dataset. Please note that the following code block might take a few seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = {label: 0 for label in labels}\n",
    "\n",
    "for _, mask in dataset:\n",
    "    arr = np.asarray(mask)\n",
    "    for i, label in enumerate(labels):\n",
    "        class_counts[label] += np.sum(arr == values[i])\n",
    "\n",
    "normalized_counts = np.array(list(class_counts.values())) / sum(list(class_counts.values()))\n",
    "\n",
    "plt.bar(class_counts.keys(), normalized_counts, tick_label=labels, color=colors)\n",
    "plt.title(\"Distribution of Pixel Counts by Class\", fontsize=title_font_size)\n",
    "plt.xlabel(\"Class\", fontsize=label_font_size)\n",
    "plt.ylabel(\"Counts (Normalized)\", fontsize=label_font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our dataset is mostly noise! A classification dataset like this—with skewed class proportions—is called imbalanced.\n",
    "\n",
    "An imbalanced dataset can result in biased and poorly performing models. Models trained on imbalanced data tends to focus more on the majority classes and may not learn enough about the minority classes. To ensure the development of a fair, accurate, and robust model, we will need to address this class imbalance. \n",
    "\n",
    "But first, let's split the dataset into separate training and validation sets. The training dataset is the portion of the dataset that will be used to train the model, while the validation dataset will be held in reserve and used to evaluate the performance of the trained model. Let's start with a simple 80/20 split, where 80% of the dataset is used for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.80\n",
    "n_train_examples = int(len(dataset) * train_split)\n",
    "n_val_examples = len(dataset) - n_train_examples\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(\n",
    "    dataset=dataset, lengths=[n_train_examples, n_val_examples], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"The training split contains {len(train_set)} examples.\")\n",
    "print(f\"The validation split contains {len(val_set)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, data loaders facilitate easy access to samples, efficiently load and batch data, and offer numerous other features to streamline data preprocessing, management, and integration within the training loop. Let's create data loaders for both the training and validation datasets.\n",
    "\n",
    "In PyTorch, the `DataLoader` class allows us to pass a `batch_size` argument, which controls the number of samples used in each pass through the network. Using a small number of training examples each pass is called mini-batching, and can improve efficiency, stabilize training dynamics, and enable scalable training on large datasets. Choosing an appropriate mini-batch size depends on several factors, including the available memory on your hardware, training efficiency constraints, and generalization requirements. However, as with everything in machine learning, we ultimately rely on empirical testing to determine the optimal configuration that maximizes model performance for each specific task and dataset. In this example, we'll use mini-batches containing 4 samples each, which will easily fit on any CPU/GPU without issue and provide reasonable generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "spects, masks = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch of spectrograms: {type(spects)}, {spects.dtype}, {spects.size()}\")\n",
    "print(f\"Batch of masks: {type(masks)}, {masks.dtype}, {masks.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a batch of spectrograms along with their corresponding masks. Please note that the following plotting code is optimized for small batch sizes and may not render as nicely with larger batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spects(spects: list[Image.Image]) -> None:\n",
    "    fig, axes = plt.subplots(figsize=[batch_size * 2, 3], nrows=1, ncols=batch_size, sharey=True)\n",
    "    fig.text(0.5, 0.75, \"Spectrograms\", fontsize=title_font_size, ha=\"center\")\n",
    "    axes[0].set_ylabel(\"Time [arb. units]\", fontsize=label_font_size)\n",
    "    fig.text(0.5, 0.12, \"Freq. [arb. units]\", fontsize=label_font_size, ha=\"center\")\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        im = ax.imshow(spects[i], vmin=0, vmax=255)\n",
    "\n",
    "    fig.subplots_adjust(right=0.90)\n",
    "    cbar_ax = fig.add_axes(rect=[0.93, 0.24, 0.02, 0.5])\n",
    "    fig.colorbar(im, cax=cbar_ax, ticks=[0, 255])\n",
    "\n",
    "\n",
    "def plot_masks(masks: list[Image.Image], prediction: bool = False, is_pruned: bool = False, is_teacher: bool = False) -> None:\n",
    "    fig, axes = plt.subplots(figsize=[batch_size * 2, 3], nrows=1, ncols=batch_size, sharey=True)\n",
    "    if prediction:\n",
    "        if is_pruned:\n",
    "            fig.text(0.5, 0.75, \"Pruned Model Predictions\", fontsize=title_font_size, ha=\"center\")\n",
    "        elif is_teacher:\n",
    "            fig.text(0.5, 0.75, \"Teacher Model Predictions\", fontsize=title_font_size, ha=\"center\")\n",
    "        else:\n",
    "            fig.text(0.5, 0.75, \"Model Predictions\", fontsize=title_font_size, ha=\"center\")\n",
    "    else:\n",
    "        fig.text(0.5, 0.75, \"Masks\", fontsize=title_font_size, ha=\"center\")\n",
    "    axes[0].set_ylabel(\"Time [arb. units]\", fontsize=label_font_size)\n",
    "    fig.text(0.5, 0.12, \"Freq. [arb. units]\", fontsize=label_font_size, ha=\"center\")\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        im = ax.imshow(masks[i], vmin=0, vmax=2, cmap=mask_cmap)\n",
    "\n",
    "    fig.subplots_adjust(right=0.90)\n",
    "    cbar_ax = fig.add_axes(rect=[0.93, 0.24, 0.02, 0.5])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, ticks=[0.33, 1, 1.66])\n",
    "    cbar.ax.set_yticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spects(spects=[inv_transform(i) for i in spects])\n",
    "plot_masks(masks=[inv_target_transform(i) for i in masks], is_pruned=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can view different batches from the dataset by rerunning the previous few code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check whether it is needed\n",
    "n_classes = 3  # We are dealing with three classes: Noise, NR, and LTE.\n",
    "median_count = statistics.median(list(class_counts.values()))\n",
    "weight = [median_count / class_counts[k] for k in class_counts.keys()]\n",
    "norm = math.sqrt(sum(x**2 for x in weight))\n",
    "normalized_list = [x / norm for x in weight]\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor(weight, dtype=torch.float))\n",
    "\n",
    "print(\"Class weights: \", {k: round(weight[i], 2) for i, k in enumerate(class_counts.keys())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the device to use for computation: a GPU if available, otherwise a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective Block Pruning\n",
    "\n",
    "This function performs selective pruning on the model, targeting one block at a time for structural pruning. The goal is to prune each block to its fullest extent while monitoring the impact on performance degradation. Here's how the process works:\n",
    "\n",
    "1. **Target One Block at a Time**: In each pruning round, only one block of the model is selected for pruning. This ensures we can isolate the effect of pruning that block on the model's performance.\n",
    "\n",
    "2. **Monitor Parameter Count**: To determine when a block has been pruned to its limit, we track the number of model parameters. Pruning continues until the parameter count stops decreasing, indicating that the block has reached its fullest possible pruning.\n",
    "\n",
    "By repeating this process for all blocks, we can analyze the importance of each block and understand how pruning affects model behavior.\n",
    "\n",
    "This approach is particularly useful for evaluating the robustness of individual blocks and identifying components that contribute the most to the model's performance. Let's explore how this is implemented!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_block_pruning(trained_model, prune_method, pruning_ratios, train_loader, device):\n",
    "    \n",
    "    # Make a copy of the trained model\n",
    "    model = copy.deepcopy(trained_model).to(device)  \n",
    "\n",
    "    blocks = []\n",
    "    model_blocks = []\n",
    "    ignored_blocks = []\n",
    "\n",
    "    # Iterate through the top-level children of the model\n",
    "    for name, module in model.named_children():\n",
    "        if name == 'classifier':\n",
    "            for idx, (backbone_name, backbone_module) in enumerate(module.named_children()):\n",
    "                blocks.append(backbone_module)\n",
    "\n",
    "        elif name == 'backbone':\n",
    "            # Iterate over the top-level children of the backbone\n",
    "            for idx, (backbone_name, backbone_module) in enumerate(module.named_children()):\n",
    "                blocks.append(backbone_module)\n",
    "\n",
    "    model_blocks.extend(blocks[1:-4])\n",
    "    ignored_blocks.extend(blocks[:1])\n",
    "    ignored_blocks.extend(blocks[-4:])\n",
    "\n",
    "    # Prepare pruning information for each block\n",
    "    pruning_info = {\n",
    "        i: {\"block\": model_blocks[i], \"pruning_ratio\": ratio}\n",
    "        for i, ratio in enumerate(pruning_ratios)\n",
    "    }\n",
    "\n",
    "    if prune_method == 'channel_pruning_Taylor_importance':\n",
    "        # Initialize TaylorImportance for pruning\n",
    "        imp = tp.importance.TaylorImportance() \n",
    "\n",
    "        # Prepare a batch from the train loader for pruning and backward pass\n",
    "        spects, masks = next(iter(train_loader))\n",
    "        spects, masks = spects.to(device), masks.to(device)\n",
    "\n",
    "        # Perform forward and backward passes to calculate importance scores\n",
    "        if isinstance(imp, tp.importance.TaylorImportance):\n",
    "            if hasattr(loss_function, 'weight') and loss_function.weight is not None:\n",
    "                loss_function.weight = loss_function.weight.to(device)\n",
    "            preds = model(spects)[\"out\"]\n",
    "            loss = loss_function(preds, masks)\n",
    "            loss.backward()\n",
    "\n",
    "        # Define layers to be ignored during pruning\n",
    "        ignored_layers = ignored_blocks\n",
    "\n",
    "        # Example input for MACs and Params calculation\n",
    "        # example_inputs = torch.randn(1, 8, 4096).to('cpu')\n",
    "        original_macs, original_nparams = tp.utils.count_ops_and_params(model, spects)\n",
    "\n",
    "        # Iterate through each block and apply pruning\n",
    "        for i, info in pruning_info.items():\n",
    "            block_to_prune = info[\"block\"]\n",
    "            pruning_ratio = info[\"pruning_ratio\"]\n",
    "\n",
    "            # Ignore all blocks except the current block to prune\n",
    "            ignored_layers_block = [pruning_info[j][\"block\"] for j in range(len(pruning_info)) if j != i]\n",
    "            combined_ignored_layers = ignored_layers + ignored_layers_block\n",
    "\n",
    "            count = 0  # Counter for consecutive iterations without parameter reduction\n",
    "            # print(f\"Pruning block {i+1}/{len(pruning_info)}\")\n",
    "            if pruning_ratio == 0:\n",
    "                continue\n",
    "            \n",
    "            # Pruning loop: Continue pruning until no parameters are further reduced\n",
    "            while True:\n",
    "\n",
    "                # print(f\"Pruning ratio: {pruning_ratio}\")\n",
    "                \n",
    "                # Apply pruning for the current block\n",
    "                pruner_group = tp.pruner.MagnitudePruner(\n",
    "                    model,\n",
    "                    example_inputs=spects,\n",
    "                    importance=imp,\n",
    "                    pruning_ratio=pruning_ratio,\n",
    "                    ignored_layers=combined_ignored_layers\n",
    "                )\n",
    "                pruner_group.step()\n",
    "\n",
    "                # Recalculate MACs and parameters after pruning\n",
    "                macs, nparams = tp.utils.count_ops_and_params(model, spects)\n",
    "                # print(f\"MACs: {macs / 1e9:.2f} G, #Params: {nparams / 1e3:.2f} K\")\n",
    "                # print(f\"Parameter reduction: {original_nparams - nparams}\")\n",
    "                \n",
    "                # Check if no parameters were reduced, then break the loop\n",
    "                if original_nparams - nparams == 0:\n",
    "                    count += 1\n",
    "                    if count == 1:\n",
    "                        pruning_ratio = 0.5  # Adjust pruning ratio\n",
    "                        # print(f\"Consecutive iterations without parameter reduction: {count}\")\n",
    "                        # print(f\"Adjusting pruning ratio to {pruning_ratio}\")\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                # Update the pruning ratio for iterative pruning\n",
    "                original_nparams = nparams\n",
    "                # pruning_ratio = 0.5  # Adjust pruning ratio for the next iteration\n",
    "\n",
    "        # Free up memory\n",
    "        del spects, preds\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return model, macs, nparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth Importance Analysis\n",
    "\n",
    "This function loops through the model blocks and applies the **selective block pruning** function to each block. The objective is to calculate the relative contribution of each block to the following metrics:\n",
    "\n",
    "1. **Performance Degradation**: The impact of pruning each block on the model's accuracy or another performance metric.\n",
    "2. **Parameter Reduction**: The reduction in the number of model parameters caused by pruning the block.\n",
    "3. **MACs Reduction**: The reduction in Multiply-Accumulate Operations (MACs) due to pruning the block.\n",
    "\n",
    "These contributions are combined into a **weighted importance score** to reflect the overall significance of each block. The weights allow us to prioritize certain metrics over others, such as placing higher importance on performance degradation while still considering parameter and MACs reduction.\n",
    "\n",
    "The importance score for each block `i` is calculated using the following equation:\n",
    "\n",
    "$$\n",
    "\\text{Imp}_i = w_{\\text{accuracy}} \\cdot \\left(\\frac{\\text{Accuracy}_{\\text{deg}, i}}{\\sum_j \\text{Accuracy}_{\\text{deg}, j}}\\right) \n",
    "+ w_{\\text{params}} \\cdot \\left(1 - \\frac{\\text{Params}_{\\text{red}, i}}{\\sum_j \\text{Params}_{\\text{red}, j}}\\right) \n",
    "+ w_{\\text{MACs}} \\cdot \\left(1 - \\frac{\\text{MACs}_{\\text{red}, i}}{\\sum_j \\text{MACs}_{\\text{red}, j}}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "- `Accuracy_deg_i`: Accuracy degradation caused by pruning block `i`.\n",
    "- `Params_red,i`: Reduction in parameters from pruning block `i`.\n",
    "- `MACs_red,i`: Reduction in MACs from pruning block `i`.\n",
    "- `w_loss`, `w_params`, `w_MACs`: Weights that determine the contribution of each term to the importance score.\n",
    "\n",
    "This analysis helps identify the blocks that are most critical to the model's performance and resource efficiency. The results can be used to guide selective pruning strategies for achieving the optimal trade-off between accuracy and model compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform perplexity\n",
    "def perplexity_analysis_with_contributions(original_model, data_loader, metric, device, criterion=None):\n",
    "\n",
    "    blocks = []\n",
    "    model_blocks = []\n",
    "\n",
    "    # Iterate through the top-level children of the model\n",
    "    for name, module in original_model.named_children():\n",
    "        if name == 'classifier':\n",
    "            for idx, (backbone_name, backbone_module) in enumerate(module.named_children()):\n",
    "                blocks.append(backbone_module)\n",
    "\n",
    "        elif name == 'backbone':\n",
    "            # Iterate over the top-level children of the backbone\n",
    "            for idx, (backbone_name, backbone_module) in enumerate(module.named_children()):\n",
    "                blocks.append(backbone_module)\n",
    "\n",
    "    model_blocks.extend(blocks[1:-4])\n",
    "    blocks_number = len(model_blocks)\n",
    "\n",
    "    total_block_performance = [0.0 for _ in range(blocks_number)] #TODO add the number dynamically\n",
    "    params_reduction = []  # Store the parameter reduction for each block\n",
    "    macs_reduction = []\n",
    "\n",
    "    # Step 1: Compute the baseline performance (without block replacement)\n",
    "    original_model.to(device)\n",
    "    print(f\"Computing baseline {metric} without block replacement...\")\n",
    "    baseline_performance = compute_baseline_performance(original_model, data_loader, device)\n",
    "    print(f\"Baseline {metric}: {baseline_performance}\")\n",
    "\n",
    "    \n",
    "    example_inputs = torch.randn(1, 3, 256, 256).to(device)  # Generate example input for calculating MACs and parameters\n",
    "    original_macs, original_nparams = tp.utils.count_ops_and_params(original_model, example_inputs)\n",
    "    \n",
    "\n",
    "    # Iterate through each block for replacement\n",
    "    for block_idx in range(blocks_number):\n",
    "\n",
    "        # output_channels = get_first_layer_output_channels(original_model)\n",
    "\n",
    "        print(f\"Replacing block {block_idx}\")\n",
    "        pruning_ratios = pruning_ratios = (np.eye(blocks_number) * 0.8)[block_idx]\n",
    "        \n",
    "        pruned_model, macs, nparams = selective_block_pruning(original_model,'channel_pruning_Taylor_importance', pruning_ratios, data_loader, device)\n",
    "\n",
    "        print(f\"Macs reduction is: {((original_macs - macs) / original_macs * 100):.2f}%\", f\"Parameters reduction is: {((original_nparams - nparams)/original_nparams*100):.2f}%\")\n",
    "\n",
    "        # Record the parameter reduction\n",
    "        params_reduction.append((original_nparams - nparams)/original_nparams * 100)\n",
    "        macs_reduction.append((original_macs - macs) / original_macs * 100)\n",
    "        \n",
    "        pruned_model.to(device)\n",
    "\n",
    "        # Run validation and compute performance\n",
    "        block_performance = compute_baseline_performance(pruned_model, data_loader, device)\n",
    "\n",
    "        total_block_performance[block_idx] = block_performance\n",
    "        print(f'The {metric} after pruning this block is: ', total_block_performance[block_idx])\n",
    "\n",
    "    # Step 2: Calculate the final averaged performance and contribution for each block across iterations\n",
    "    total_degradation_in_performance = 0.0\n",
    "    block_degradation = []\n",
    "    total_params_reduction = 0.0\n",
    "    total_macs_reduction = 0.0\n",
    "    \n",
    "    for block_idx in range(blocks_number):\n",
    "        final_average_performance = total_block_performance[block_idx]\n",
    "        degradation_in_performance = np.abs(final_average_performance - baseline_performance)\n",
    "        print(f\"degradation in {metric} is: \", degradation_in_performance)\n",
    "        block_degradation.append(degradation_in_performance)\n",
    "        total_degradation_in_performance += degradation_in_performance  # Accumulate total degradation in performance\n",
    "        total_params_reduction += params_reduction[block_idx]  # Accumulate total parameter reduction\n",
    "        total_macs_reduction += macs_reduction[block_idx]  # Accumulate total macs reduction\n",
    "    \n",
    "    # Step 3: Calculate the relative contribution of each block to the total degradation in performance and params saved\n",
    "    relative_contributions = []\n",
    "    weighted_importance_scores = []\n",
    "    print(f\"\\nRelative contribution of each block to total {metric} degradation and parameter reduction:\")\n",
    "\n",
    "    for block_idx in range(blocks_number):\n",
    "        # Calculate relative contribution to the performance degradation\n",
    "        relative_contribution_performance = ((block_degradation[block_idx] / total_degradation_in_performance))* 100\n",
    "        \n",
    "        # Adjust parameter contribution to reflect reduced importance for larger reductions\n",
    "        relative_contribution_params = 100 - params_reduction[block_idx] #(1 - (params_reduction[block_idx] / total_params_reduction)) * 100\n",
    "        relative_contribution_macs = 100 - macs_reduction[block_idx] #(1 - (macs_reduction[block_idx] / total_macs_reduction)) * 100\n",
    "\n",
    "        # Combine these two using a weighted importance score (example: 70% weight to performance, 30% to parameter savings)\n",
    "        weight_performance = 0.5\n",
    "        weight_params = 0.3\n",
    "        weight_macs = 0.2\n",
    "        weighted_importance = (weight_performance * relative_contribution_performance) + (weight_params * relative_contribution_params) + (weight_macs * relative_contribution_macs)\n",
    "        \n",
    "        print(f'Block {block_idx} contributes {relative_contribution_performance:.2f}% to the total degradation in {metric} and reduces {params_reduction[block_idx]:.2f}% of parameters.')\n",
    "        print(f'Weighted importance score for Block {block_idx}: {weighted_importance:.2f}')\n",
    "        \n",
    "        relative_contributions.append(relative_contribution_performance)\n",
    "        weighted_importance_scores.append(weighted_importance)\n",
    "\n",
    "    # Return both the relative performance contributions and weighted importance scores\n",
    "    return weighted_importance_scores\n",
    "\n",
    "# Helper function to compute the baseline performance (without replacing any block)\n",
    "def compute_baseline_performance(model, data_loader, device):\n",
    "\n",
    "    metrics = [\n",
    "        MulticlassAccuracy(num_classes=n_classes, average=None),\n",
    "        MulticlassRecall(num_classes=n_classes, average=None),\n",
    "        MulticlassPrecision(num_classes=n_classes, average=None),\n",
    "        MulticlassF1Score(num_classes=n_classes, average=None),\n",
    "        # The IoU is commonly referred to as Jaccard's Index\n",
    "        MulticlassJaccardIndex(num_classes=n_classes, average=None),\n",
    "    ]\n",
    "    metrics = [m.to(device) for m in metrics]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for spect, mask in data_loader:\n",
    "            spect, mask = spect.to(device), mask.to(device)\n",
    "            pred = (model(spect)[\"out\"]).argmax(dim=1)\n",
    "            for m in metrics:\n",
    "                m.update(pred, mask)\n",
    "\n",
    "    metrics = [m.compute().cpu().numpy() for m in metrics]\n",
    "    metrics = [np.append(m, statistics.mean(m)) for m in metrics]\n",
    "\n",
    "    del spect, mask\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return metrics[3][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pre-trained model which will be used as the teacher model later in Knowledge Distillation\n",
    "trained_model = torch.load('./trained_model/best_model.pth')\n",
    "relative_contribution = perplexity_analysis_with_contributions(trained_model, val_loader, metric='accuracy', device=general_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block-Wise Pruning Ratio Calculation\n",
    "\n",
    "This function calculates a pruning ratio `p_block_i` for each block `i` using a non-linear scaling method. The approach is based on the exponential decay of the **weighted depth importance scores**, ensuring that:\n",
    "\n",
    "- Blocks with **lower importance scores** are pruned more aggressively.\n",
    "- Blocks with **higher importance scores** are pruned more conservatively, preserving their contributions to the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pruning_ratios(contributions, max_pruning_ratio=0.9, k=5):\n",
    "    \"\"\"\n",
    "    Calculate pruning ratios based on intense nonlinear scaling (exponential decay) of the relative contributions.\n",
    "\n",
    "    Parameters:\n",
    "    - contributions (list): List of relative contributions (in percentages) of each block to total loss increase.\n",
    "    - max_pruning_ratio (float): Maximum pruning ratio to be assigned to the least important layer. Default is 0.9.\n",
    "    - k (int): Factor controlling the intensity of the scaling (larger k makes the ratio more intense).\n",
    "\n",
    "    Returns:\n",
    "    - pruning_ratios (list): List of pruning ratios for each block.\n",
    "    \"\"\"\n",
    "    # Normalize the contributions to get values between 0 and 1\n",
    "    total_contribution = sum(contributions)\n",
    "    normalized_contributions = [contribution / total_contribution for contribution in contributions]\n",
    "\n",
    "    # Apply exponential decay to magnify the effect for less important blocks\n",
    "    pruning_factors = [np.exp(-k * nc) for nc in normalized_contributions]\n",
    "\n",
    "    # Normalize the pruning factors so they stay within the max pruning ratio\n",
    "    max_factor = max(pruning_factors)\n",
    "    normalized_factors = [pf / max_factor for pf in pruning_factors]\n",
    "\n",
    "    # Scale by the maximum pruning ratio\n",
    "    pruning_ratios = [max_pruning_ratio * nf for nf in normalized_factors]\n",
    "\n",
    "    pruning_ratios = [round(num, 2) for num in pruning_ratios]\n",
    "\n",
    "    return pruning_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pruning_ratio = 0.99 # Maximum pruning ratio (99%) \n",
    "k = 5 # Controls the intensity of the scaling\n",
    "\n",
    "pruning_ratios = calculate_pruning_ratios(relative_contribution, max_pruning_ratio, k)\n",
    "\n",
    "# Print the pruning ratios for each block\n",
    "for i, ratio in enumerate(pruning_ratios):\n",
    "    print(f\"Block {i} Pruning Ratio: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3  # We are dealing with three classes: Noise, NR, and LTE.\n",
    "median_count = statistics.median(list(class_counts.values()))\n",
    "weight = [median_count / class_counts[k] for k in class_counts.keys()]\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor(weight, dtype=torch.float))\n",
    "\n",
    "print(\"Class weights: \", {k: round(weight[i], 2) for i, k in enumerate(class_counts.keys())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pruning\n",
    "\n",
    "This function performs **structural selective pruning** on the model blocks based on the pruning ratios `p_block_i` calculated in the previous step. \n",
    "\n",
    "The process is similar to the block selective pruning function but incorporates the pruning ratios to determine how much each block should be pruned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(trained_model, prune_method, pruning_ratios, train_loader, device, seed=42):\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Make a copy of the trained model\n",
    "    model = copy.deepcopy(trained_model).to(device)  # Assuming general_device is defined elsewhere\n",
    "\n",
    "    blocks = []\n",
    "    model_blocks = []\n",
    "    ignored_blocks = []\n",
    "\n",
    "    # Iterate through the top-level children of the model\n",
    "    for name, module in model.named_children():\n",
    "        if name == 'classifier':\n",
    "            for idx, (backbone_name, backbone_module) in enumerate(module.named_children()):\n",
    "                blocks.append(backbone_module)\n",
    "\n",
    "        elif name == 'backbone':\n",
    "            # Iterate over the top-level children of the backbone\n",
    "            for idx, (backbone_name, backbone_module) in enumerate(module.named_children()):\n",
    "                blocks.append(backbone_module)\n",
    "\n",
    "    model_blocks.extend(blocks[1:-4])\n",
    "    ignored_blocks.extend(blocks[:1])\n",
    "    ignored_blocks.extend(blocks[-4:])\n",
    "\n",
    "    # Prepare pruning information for each block\n",
    "    pruning_info = {\n",
    "        i: {\"block\": model_blocks[i], \"pruning_ratio\": ratio}\n",
    "        for i, ratio in enumerate(pruning_ratios)\n",
    "    }\n",
    "\n",
    "    if prune_method == 'channel_pruning_Taylor_importance':\n",
    "        # Initialize TaylorImportance for pruning\n",
    "        imp = tp.importance.TaylorImportance() \n",
    "\n",
    "        # Prepare a batch from the train loader for pruning and backward pass\n",
    "        spects, masks = next(iter(train_loader))\n",
    "        spects, masks = spects.to(device), masks.to(device)\n",
    "\n",
    "        # Perform forward and backward passes to calculate importance scores\n",
    "        if isinstance(imp, tp.importance.TaylorImportance):\n",
    "            if hasattr(loss_function, 'weight') and loss_function.weight is not None:\n",
    "                loss_function.weight = loss_function.weight.to(device)\n",
    "            preds = model(spects)[\"out\"]\n",
    "            loss = loss_function(preds, masks)\n",
    "            loss.backward()\n",
    "\n",
    "        # Define layers to be ignored during pruning\n",
    "        ignored_layers = ignored_blocks\n",
    "\n",
    "        # Example input for MACs and Params calculation\n",
    "        # example_inputs = torch.randn(1, 8, 4096).to('cpu')\n",
    "        original_macs, original_nparams = tp.utils.count_ops_and_params(model, spects)\n",
    "\n",
    "        # Iterate through each block and apply pruning\n",
    "        for i, info in pruning_info.items():\n",
    "            block_to_prune = info[\"block\"]\n",
    "            pruning_ratio = info[\"pruning_ratio\"]\n",
    "\n",
    "            # Ignore all blocks except the current block to prune\n",
    "            ignored_layers_block = [pruning_info[j][\"block\"] for j in range(len(pruning_info)) if j != i]\n",
    "            combined_ignored_layers = ignored_layers + ignored_layers_block\n",
    "\n",
    "            print(f\"Pruning block {i} with initial ratio: {pruning_ratio}\")\n",
    "            \n",
    "            # Apply pruning for the current block\n",
    "            pruner_group = tp.pruner.MagnitudePruner(\n",
    "                model,\n",
    "                example_inputs=spects,\n",
    "                importance=imp,\n",
    "                pruning_ratio=pruning_ratio,\n",
    "                ignored_layers=combined_ignored_layers\n",
    "            )\n",
    "            pruner_group.step()\n",
    "\n",
    "    # Recalculate MACs and parameters after pruning\n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, spects)\n",
    "    print(f\"MACs: {macs / 1e9:.2f} G, #Params: {nparams / 1e3:.2f} K\")\n",
    "    print(f\"Parameter reduction: {original_nparams - nparams}\")\n",
    "            \n",
    "\n",
    "    # Free up memory\n",
    "    del spects, masks\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, macs, nparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model, macs, nparams = prune_model(trained_model,'channel_pruning_Taylor_importance', pruning_ratios, val_loader, general_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self_SegmentationSGD: A Flexible Training Framework\n",
    "\n",
    "The `Self_SegmentationSGD` Lightning class provides a versatile framework for training segmentation models, supporting **knowledge distillation**, **self-knowledge distillation**, and **conventional fine-tuning**. By adjusting the weights of the loss components, you can control the training strategy to suit your specific needs.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Knowledge Distillation (KD)**:\n",
    "   - Leverages a pre-trained **teacher model** to guide the student model during training.\n",
    "   - Uses soft targets from the teacher model to compute a distillation loss, weighted by `alpha`.\n",
    "\n",
    "2. **Self-Knowledge Distillation**:\n",
    "   - Employs an **Exponential Moving Average (EMA) model** as a self-teacher.\n",
    "   - The EMA model provides pseudo-labels to compute the self-distillation loss, weighted by `gamma_self`.\n",
    "\n",
    "3. **Conventional Fine-Tuning**:\n",
    "   - Optimizes the model using hard labels and a standard loss function, weighted by `beta`.\n",
    "\n",
    "### Loss Function Components:\n",
    "\n",
    "The total loss combines three components as follows:\n",
    "\n",
    "$$\n",
    "\\text{Total Loss} = \\alpha \\cdot \\text{Distillation Loss} + \\beta \\cdot \\text{Hard Loss} + \\gamma_{\\text{self}} \\cdot \\text{Self-Distillation Loss}\n",
    "$$\n",
    "- **Distillation Loss**: KL divergence between the student's predictions and the teacher's soft targets.\n",
    "- **Hard Loss**: Standard segmentation loss computed with ground-truth labels.\n",
    "- **Self-Distillation Loss**: KL divergence between the student's predictions and the EMA model's soft targets.\n",
    "\n",
    "### Customizability:\n",
    "\n",
    "By tuning the weights `alpha`, `beta`, and `gamma_self`:\n",
    "- You can focus more on knowledge distillation (higher`alpha`).\n",
    "- Emphasize self-knowledge distillation (higher `gamma_self`).\n",
    "- Rely primarily on conventional fine-tuning (higher `beta`).\n",
    "\n",
    "This flexible architecture enables a seamless transition between different training paradigms, making it an ideal choice for scenarios requiring efficient segmentation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_SegmentationSGD(L.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: nn.Module,\n",
    "        loss_function: nn.Module,\n",
    "        teacher_model: nn.Module,\n",
    "        n_classes: int,\n",
    "        learning_rate: float,\n",
    "        momentum: float,\n",
    "        weight_decay: float,\n",
    "        step_size: int,\n",
    "        gamma: float,\n",
    "        alpha: float,  # Trade-off factor for KD loss\n",
    "        beta: float,   # Trade-off factor for hard loss \n",
    "        gamma_self: float,  # Trade-off factor for self-distillation loss\n",
    "        temperature: float,  # Temperature for distillation\n",
    "        class_weights: float,\n",
    "        ema_decay: float  # Decay rate for EMA\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.teacher_model = teacher_model\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma_self = gamma_self\n",
    "        self.temperature = temperature\n",
    "        self.ema_decay = ema_decay\n",
    "\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float, device='cuda').view(1, 3, 1, 1)\n",
    "\n",
    "        self.train_accuracy = MulticlassAccuracy(num_classes=self.n_classes)\n",
    "        self.val_accuracy = MulticlassAccuracy(num_classes=self.n_classes)\n",
    "\n",
    "        # Freeze the teacher model \n",
    "        self.teacher_model.eval()\n",
    "        for param in self.teacher_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Initialize EMA model\n",
    "        self.ema_model = copy.deepcopy(self.model)\n",
    "        self.ema_model.eval()  # EMA model is always in eval mode\n",
    "        for param in self.ema_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Defines a forward pass through the student model.\"\"\"\n",
    "        return self.model(x)            \n",
    "\n",
    "    def update_ema_model(self):\n",
    "        \"\"\"Updates the EMA model's parameters using the current model's parameters.\"\"\"\n",
    "        for ema_param, model_param in zip(self.ema_model.parameters(), self.model.parameters()):\n",
    "            ema_param.data = self.ema_decay * ema_param.data + (1.0 - self.ema_decay) * model_param.data\n",
    "\n",
    "    def training_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        \"\"\"Defines a single training step with knowledge distillation and self-distillation.\"\"\"\n",
    "        image, target = batch\n",
    "\n",
    "        # Student predictions\n",
    "        student_preds = self(image)[\"out\"]\n",
    "\n",
    "        # Teacher predictions (soft targets)\n",
    "        with torch.no_grad():\n",
    "            teacher_preds = self.teacher_model(image)[\"out\"]\n",
    "\n",
    "        # Compute teacher soft targets\n",
    "        teacher_soft_targets = softmax(teacher_preds / self.temperature, dim =1)\n",
    "        student_soft_targets = log_softmax(student_preds / self.temperature, dim=1)\n",
    "\n",
    "        # Distillation loss\n",
    "        kl_per_class = kl_div(student_soft_targets, teacher_soft_targets, reduction=\"none\")\n",
    "        kl_weighted = kl_per_class * self.class_weights\n",
    "        distillation_loss = kl_weighted.sum(dim=1).mean() * (self.temperature ** 2)\n",
    "\n",
    "        # Hard target loss\n",
    "        hard_loss = self.loss_function(student_preds, target)\n",
    "\n",
    "        # Self-distillation loss (using EMA model as pseudo-labels)\n",
    "        with torch.no_grad():\n",
    "            ema_preds = self.ema_model(image)[\"out\"]  # EMA model predictions\n",
    "            ema_soft_targets = softmax(ema_preds / self.temperature, dim=1)\n",
    "\n",
    "        student_self_targets = log_softmax(student_preds / self.temperature, dim=1)\n",
    "        self_kl_per_class = kl_div(student_self_targets, ema_soft_targets, reduction=\"none\")\n",
    "        self_kl_weighted = self_kl_per_class * self.class_weights\n",
    "        self_distillation_loss = self_kl_weighted.sum(dim=1).mean() * (self.temperature ** 2)\n",
    "\n",
    "        # Combined loss\n",
    "        loss = self.alpha * distillation_loss + self.beta * hard_loss + self.gamma_self * self_distillation_loss\n",
    "\n",
    "        # Log metrics\n",
    "        self.train_accuracy(student_preds, target)\n",
    "        self.log(name=\"train_accuracy\", value=self.train_accuracy, prog_bar=True)\n",
    "        self.log(name=\"train_loss\", value=loss, on_epoch=True, on_step=False, prog_bar=True)\n",
    "        self.log(name=\"distillation_loss\", value=distillation_loss, on_epoch=True, on_step=True)\n",
    "        self.log(name=\"hard_loss\", value=hard_loss, on_epoch=True, on_step=True)\n",
    "        self.log(name=\"self_distillation_loss\", value=self_distillation_loss, on_epoch=True, on_step=True)\n",
    "\n",
    "        # Update EMA model after the loss computation\n",
    "        self.update_ema_model()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        \"\"\"Defines a single validation step.\"\"\"\n",
    "        image, target = batch\n",
    "        preds = self(image)[\"out\"]\n",
    "        loss = self.loss_function(preds, target)\n",
    "        self.val_accuracy(preds, target)\n",
    "        self.log(name=\"val_accuracy\", value=self.val_accuracy, prog_bar=True)\n",
    "        self.log(name=\"val_loss\", value=loss, on_epoch=True, on_step=False, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> dict[str, Any]:\n",
    "        \"\"\"Configure the optimizer and learning rate scheduler.\"\"\"\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(), lr=self.learning_rate, momentum=self.momentum, weight_decay=self.weight_decay\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=self.step_size, gamma=self.gamma)\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some checkpoints to save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"retrained_compressed_model\"\n",
    "model_path = os.path.join(save_dir, 'best_model.pth')\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "class CustomCheckpoint(L.Callback): #TODO change it to track accuracy instead\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model = None\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        # Access validation loss from the trainer's metrics\n",
    "        val_loss = trainer.callback_metrics.get('val_loss')\n",
    "        if val_loss is not None and val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.best_model = pl_module.model\n",
    "            # Save the best model\n",
    "            torch.save(self.best_model, model_path)\n",
    "            print(f\"New best model saved with validation loss {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "import lightning.pytorch #TODO figure out the difference between this and L\n",
    "\n",
    "checkpoint_filename = f\"{save_dir}/final_original_model.ckpt\"\n",
    "checkpoint_callback = lightning.pytorch.callbacks.ModelCheckpoint(\n",
    "        dirpath='.',\n",
    "        filename=checkpoint_filename.replace(\".ckpt\", \"\"),\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "# Create the custom callback\n",
    "custom_checkpoint = CustomCheckpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_segmentation_module = Self_SegmentationSGD(\n",
    "    model=pruned_model,\n",
    "    loss_function=loss_function,\n",
    "    teacher_model=trained_model,\n",
    "    n_classes=n_classes,\n",
    "    learning_rate=0.001,  # Represents the initial learning rate.\n",
    "    momentum=0.9,\n",
    "    weight_decay=1.0e-04,\n",
    "    step_size=10,\n",
    "    gamma=0.1,\n",
    "    gamma_self=0,\n",
    "    alpha=1,\n",
    "    beta=0,\n",
    "    temperature=2,\n",
    "    class_weights=weight, \n",
    "    ema_decay=0.999\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "# n_epochs = 4  # Suggested for CPU training.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Training model on GPU.\")\n",
    "    trainer = L.Trainer(accelerator=\"gpu\", devices=1, callbacks=[custom_checkpoint], max_epochs=n_epochs, logger=True)\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"Training model on CPU.\")\n",
    "    trainer = L.Trainer(max_epochs=n_epochs, logger=True)\n",
    "    device = \"cpu\"\n",
    "\n",
    "trainer.fit(model=self_segmentation_module, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can find the best model retrained with knowledge distillation saved in the `KD_trained_model` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "\n",
    "Having trained our model, the next step is to evaluate its performance. To accomplish this, we'll use a suite of standard machine learning metrics. But first, let's take a look at a random batch of predictions.\n",
    "\n",
    "Because the model returns the probabilities corresponding to the predictions of each class. We need to use `argmax()` to obtain the class with the highest prediction probability. The result is a singe-channel image for each example in the batch, which can be compared directly to the corresponding target mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = torch.load('./trained_model/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./retrained_compressed_model/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "spects, masks = next(iter(train_loader))\n",
    "spects = spects.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = (model(spects)[\"out\"]).argmax(1)\n",
    "\n",
    "print(\"Predictions:\", preds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spects(spects=[inv_transform(i.cpu()) for i in spects])\n",
    "plot_masks(masks=[inv_target_transform(i) for i in masks])\n",
    "plot_masks(masks=preds.cpu(), prediction=True, is_pruned=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good! But to get a more objective sense, let's turn to the metrics. Let's start with model accuracy, calculated as the ratio of correctly predicted pixels to the total number of pixels.\n",
    "\n",
    "**Note:** You can view different predictions by rerunning the previous few code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_segmentation_module = Self_SegmentationSGD(\n",
    "    model=model,\n",
    "    loss_function=loss_function,\n",
    "    teacher_model=teacher_model,\n",
    "    n_classes=n_classes,\n",
    "    learning_rate=0.001,  # Represents the initial learning rate.\n",
    "    momentum=0.9,\n",
    "    weight_decay=1.0e-04,\n",
    "    step_size=10,\n",
    "    gamma=0.1,\n",
    "    gamma_self=1,\n",
    "    alpha=1,\n",
    "    beta=1,\n",
    "    temperature=2,\n",
    "    class_weights=weight, #[x**2 for x in weight] #weight\n",
    "    ema_decay=0.999\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',  # Use GPU\n",
    "    devices=1,          # Single GPU\n",
    "    logger=True         # Logging enabled\n",
    ")\n",
    "scores = trainer.validate(model=self_segmentation_module, dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy can give us a quick sense of the model's overall performance. However, accuracy alone doesn't tell the whole story. In fact, because of the imbalance in our dataset, a reasonably high accuracy could be achieved by always predicting  noise. The simple accuracy provided above is an unweighted mean of the accuracies over each class.\n",
    "\n",
    "To gain a better understanding of our model's ability to predict specific classes, let's take a look at the confusion matrix, which provides a more comprehensive overview of model capability. The diagonal elements represent the correct predictions and off-diagonal elements indicate prediction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "confusion_matrix = MulticlassConfusionMatrix(num_classes=n_classes, normalize=\"true\").to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for spect, mask in val_loader:\n",
    "        spect, mask = spect.to(device), mask.to(device)\n",
    "        pred = (model(spect)[\"out\"]).argmax(dim=1)\n",
    "        confusion_matrix.update(pred, mask)\n",
    "\n",
    "confusion_matrix = np.round(confusion_matrix.compute().cpu().numpy(), decimals=2)\n",
    "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=labels).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's generate a more comprehensive report, complete with the following metrics:\n",
    "\n",
    "- **Recall:** The recall (sensitivity) measures the ability of the model to identify the relevant pixels. A higher recall indicates that the model is better at identifying signal.\n",
    "\n",
    "- **Precision:** The precision assesses the accuracy of positive predictions. A higher precision indicates that when the model predicts signal, it is more likely to be correct.\n",
    "\n",
    "- **F1 Score:** The F1 score combines both recall and precision into a single value, providing a more balanced measure of the model's performance. A higher F1 indicates a model with both good precision and recall (fewer false positives and false negatives overall).\n",
    "\n",
    "- **Intersection over Union (IoU):** The IoU quantifies the overlap between the predicted bounding box or segmented region and the ground truth. A higher IoU value indicates a better alignment between the predicted and actual regions, reflecting a more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_report(dataloader: DataLoader, model) -> None:\n",
    "    \"\"\"Compute accuracy, recall, precision, F1 score, and IoU (Intersection over Union),\n",
    "    and print a report containing these metrics.\"\"\"\n",
    "    metrics = [\n",
    "        MulticlassAccuracy(num_classes=n_classes, average=None),\n",
    "        MulticlassRecall(num_classes=n_classes, average=None),\n",
    "        MulticlassPrecision(num_classes=n_classes, average=None),\n",
    "        MulticlassF1Score(num_classes=n_classes, average=None),\n",
    "        # The IoU is commonly referred to as Jaccard's Index\n",
    "        MulticlassJaccardIndex(num_classes=n_classes, average=None),\n",
    "    ]\n",
    "    metrics = [m.to(device) for m in metrics]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for spect, mask in dataloader:\n",
    "            spect, mask = spect.to(device), mask.to(device)\n",
    "            pred = (model(spect)[\"out\"]).argmax(dim=1)\n",
    "            for m in metrics:\n",
    "                m.update(pred, mask)\n",
    "\n",
    "    metrics = [m.compute().cpu().numpy() for m in metrics]\n",
    "    metrics = [np.append(m, statistics.mean(m)) for m in metrics]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Class\": np.append(np.asarray(labels), \"Mean\"),\n",
    "            \"Accuracy\": metrics[0],\n",
    "            \"Recall\": metrics[1],\n",
    "            \"Precision\": metrics[2],\n",
    "            \"F1 Score\": metrics[3],\n",
    "            \"IoU\": metrics[4],\n",
    "        }\n",
    "    )\n",
    "    print(\n",
    "        tabulate(\n",
    "            df, headers=\"keys\", tablefmt=\"grid\", showindex=False, numalign=\"center\", stralign=\"center\", floatfmt=\".2f\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics_report(dataloader=val_loader, model=teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics_report(dataloader=val_loader, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Data\n",
    "\n",
    "In machine learning, out-of-distribution data refers to examples that deviate from those used during training. For example, recall the Spectrogram Sensing dataset comprises 900 combined frames featuring both LTE and NR signals. As we excluded the combined frames from the training process, they represent out-of-distribution data. To get a quick sense of how our model performs on these combined frames, let's take a look at a random batch of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_data = os.path.join(data_root, \"LTE_NR\")\n",
    "challenge_dataset = SpectrumSensing(root=challenge_data, transform=transform, target_transform=target_transform)\n",
    "\n",
    "challenge_loader = DataLoader(challenge_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spects, masks = next(iter(challenge_loader))\n",
    "spects = spects.to(device)\n",
    "torch.manual_seed(20)\n",
    "with torch.inference_mode():\n",
    "    teacher_preds = (teacher_model(spects)[\"out\"]).argmax(1)\n",
    "    preds = (model(spects)[\"out\"]).argmax(1)\n",
    "\n",
    "print(\"Predictions:\", preds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spects(spects=[inv_transform(i.cpu()) for i in spects])\n",
    "plot_masks(masks=[inv_target_transform(i) for i in masks])\n",
    "plot_masks(masks=teacher_preds.cpu(), prediction=True, is_teacher=True)\n",
    "plot_masks(masks=preds.cpu(), prediction=True, is_pruned=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can view different examples by rerunning the previous few code cells.\n",
    "\n",
    "Now, let's evaluate the same metrics as we did above in the [Model Validation](#Model-Validation) section, but now for the challenge dataset. Given the model's lack of exposure to these combined frames during the training process, we anticipate the model's capabilities to be somewhat diminished. Yet, we still anticipate reasonable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics_report(dataloader=challenge_loader, model=teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics_report(dataloader=challenge_loader, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model: torch.nn.Module, validation_loader: torch.utils.data.DataLoader, num_tests: int = 5, num_iterations: int = 20) -> None:\n",
    "    \"\"\"\n",
    "    Measure the inference time of a PyTorch model on a CPU for multiple tests and iterations.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model.\n",
    "        validation_loader (torch.utils.data.DataLoader): The DataLoader for the validation dataset.\n",
    "        num_tests (int): The number of tests to run the inference.\n",
    "        num_iterations (int): The number of iterations to run the inference for each test.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the total and average inference time for each test and overall statistics.\n",
    "    \"\"\"\n",
    "    # Move the model to CPU and set it to evaluation mode\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculations for inference\n",
    "    overall_total_time = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test in range(num_tests):\n",
    "            print(f\"Running Test {test + 1}/{num_tests}\")\n",
    "            test_total_time = 0.0\n",
    "\n",
    "            for iteration in range(num_iterations):\n",
    "                print(f\"  Iteration {iteration + 1}/{num_iterations}\")\n",
    "                iteration_inference_time = 0.0\n",
    "\n",
    "                for batch in validation_loader:\n",
    "                    # Move the batch to the CPU (if tensors are not already on CPU)\n",
    "                    batch = [tensor.to(device) for tensor in batch]\n",
    "\n",
    "                    # Start timing\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    # Perform inference\n",
    "                    inputs = batch[0]  # Assuming the first element in the batch is the input tensor\n",
    "                    _ = model(inputs)\n",
    "\n",
    "                    # End timing\n",
    "                    batch_inference_time = time.time() - start_time\n",
    "                    iteration_inference_time += batch_inference_time\n",
    "\n",
    "                # Add the inference time for this iteration to the total for the test\n",
    "                test_total_time += iteration_inference_time\n",
    "                print(f\"    Inference time for iteration {iteration + 1}: {iteration_inference_time:.4f} seconds\")\n",
    "\n",
    "            # Calculate and print the average inference time per batch for this test\n",
    "            avg_test_time = test_total_time / (len(validation_loader) * num_iterations)\n",
    "            overall_total_time += test_total_time\n",
    "\n",
    "            print(f\"Total inference time for Test {test + 1}: {test_total_time:.4f} seconds\")\n",
    "            print(f\"Average inference time per batch for Test {test + 1}: {avg_test_time:.4f} seconds\\n\")\n",
    "\n",
    "    # Calculate and print overall statistics\n",
    "    overall_avg_time = overall_total_time / (num_tests * len(validation_loader) * num_iterations)\n",
    "    print(f\"Overall total inference time for {num_tests} tests: {overall_total_time:.4f} seconds\")\n",
    "    print(f\"Overall average inference time per batch: {overall_avg_time:.4f} seconds\")\n",
    "\n",
    "# Example usage\n",
    "# avg_time = measure_inference_time(model, validation_loader, num_tests=3, num_iterations=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model: torch.nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the size of a PyTorch model in megabytes (MB).\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "        float: The size of the model in megabytes (MB).\n",
    "    \"\"\"\n",
    "    # Initialize total size in bytes\n",
    "    total_size_in_bytes = 0\n",
    "\n",
    "    # Iterate through all parameters and buffers to calculate size\n",
    "    for param in model.parameters():\n",
    "        total_size_in_bytes += param.nelement() * param.element_size()  # nelement: total number of elements in tensor\n",
    "\n",
    "    for buffer in model.buffers():\n",
    "        total_size_in_bytes += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    # Convert bytes to megabytes (1 MB = 1024 * 1024 bytes)\n",
    "    total_size_in_mb = total_size_in_bytes / (1024 ** 2)\n",
    "\n",
    "    return total_size_in_mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = get_model_size(model)\n",
    "print(f\"Model Size: {model_size:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spectrogram-segmentation",
   "language": "python",
   "name": "spectrogram-segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
