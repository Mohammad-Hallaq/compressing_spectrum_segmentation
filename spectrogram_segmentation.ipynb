{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram Segmentation\n",
    "\n",
    "In this example, we use [PyTorch](https://pytorch.org/) and [Lightning](https://lightning.ai/docs/pytorch/stable/) to train a deep learning model to identify and differentiate between 5G NR and 4G LTE signals within wideband spectrograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "**[Background](#Background):** Delve into the problem background and learn more about the machine learning frameworks, tools, and datasets used in this example.\n",
    "\n",
    "**[Set-up](#Set-Up):** Install the libraries necessary to run the code in this notebook.\n",
    "\n",
    "**[Data Preprocessing](#Data-Preprocessing):** Load and analyze the Spectrum Sensing dataset.\n",
    "\n",
    "**[Model Training](#Model-Training):** Configure and train a Deeplabv3 model with a MobileNetV3 backbone.\n",
    "\n",
    "**[Model Validation](#Model-Validation):** Assess the performance of the model using a suite of common machine learning metrics\n",
    "\n",
    "**[Challenge Data](#Challenge-Data):** Challenge the model on combined frames containing both LTE and NR signal.\n",
    "\n",
    "**[Conclusions & Next Steps](#Conclusions-&-Next-Steps):** Interpret the results, summarize key learnings, and identify steps for expanding upon this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "5G NR (New Radio) and 4G LTE (Long-Term Evolution) are both cellular network technologies, but they represent \n",
    "different generations of mobile network standards. The ability to identify and distinguish between the two holds significant \n",
    "applications in [spectrum sensing](https://iopscience.iop.org/article/10.1088/1742-6596/2261/1/012016) and serves as a foundational example showcasing the near-term feasibility of \n",
    "[intelligent radio](https://www.qoherent.ai/intelligentradio/) technology.\n",
    "\n",
    "A spectrogram, which depicts the frequency spectrum of a signal over time, is essentially just an image. Therefore, we can\n",
    "apply state-of-the-art [semantic segmentation](https://www.ibm.com/topics/semantic-segmentation) techniques from \n",
    "the field of computer vision to the problem of spectrogram analysis. Our task is to assign one of the \n",
    "following labels to each pixel in the spectrogram: 'LTE', 'NR', or 'Noise'. ('Noise' refers to the absence of signal, representing \n",
    "a vacant or empty spectrum, also known as whitespace.)\n",
    ".\n",
    "\n",
    "The machine learning model utilized in this example is a DeepLabV3 model with a MobileNetV3 large backbone. The DeepLabv3 framework was originally introduced by Chen _et al._ in their 2017 paper titled '[Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587) and the MobileNetV3 backbone was developed by Howard _et al._ and is further discussed in their 2019 paper titled '[Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)'. For an accessible introduction to the DeepLabV3 framework, please check out Isaac Berrios' article: [DeepLabv3: Building Blocks for Robust Segmentation Models](https://medium.com/@itberrios6/deeplabv3-c0c8c93d25a4).\n",
    "\n",
    "The dataset used in this example is the Spectrum Sensing dataset, provided by MathWorks. This dataset contains 900 LTE frames, 900 NR frames, and 900 combined frames with both LTE and NR signal. In this example, we train exclusively on the individual LTE and NR examples, excluding the combined frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up\n",
    "\n",
    "In this section, we will install the dependencies required to run the code in this notebook. These dependencies include libraries and packages for tasks such as data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import statistics\n",
    "from typing import Any, Optional\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib.colors import ListedColormap\n",
    "from osgeo import gdal\n",
    "from PIL import Image\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassConfusionMatrix,\n",
    "    MulticlassF1Score,\n",
    "    MulticlassJaccardIndex,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    ")\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.models.segmentation import (  # noqa: F401\n",
    "    deeplabv3_mobilenet_v3_large,\n",
    "    deeplabv3_resnet50,\n",
    "    deeplabv3_resnet101,\n",
    ")\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    PILToTensor,\n",
    "    ToDtype,\n",
    "    ToPILImage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_font_size, label_font_size = 14, 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In semantic segmentation, the input data typically consists of images (in this case, spectrograms), while the output data consists of pixel-wise labels (masks) where each pixel is assigned a category label (in this case, either 'LTE', 'NR', or 'Noise'). \n",
    "\n",
    "We will use [supervised learning](https://www.ibm.com/topics/supervised-learning) techniques to train our model. These techniques require both input spectrograms and the corresponding target masks for training. For each frame in the dataset, we have two separate files:\n",
    "\n",
    "- A `.png` file containing the spectrogram image to use as input to the model.\n",
    "\n",
    "- A `.hdf` ([HDF4](https://www.hdfgroup.org/solutions/hdf4/)) file containing the target mask to use for training.\n",
    "\n",
    "The Spectrum Sensing dataset also includes `.mat` files containing metadata such as the signal sample rate. However, none of this metadata is necessary for this example, so we can safely ignore these files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because spectrogram segmentation is a computer vision task, let's extend the [VisionDataset](https://pytorch.org/vision/main/generated/torchvision.datasets.VisionDataset.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrumSensing(VisionDataset):\n",
    "\n",
    "    def __init__(self, root: str, transform: Optional[callable] = None, target_transform: Optional[callable] = None):\n",
    "        \"\"\"Initialize the dataset, specifying the root directory where the dataset files are located.\"\"\"\n",
    "        super().__init__(root)\n",
    "\n",
    "        self.root = root\n",
    "        self.transform, self.target_transform = transform, target_transform\n",
    "\n",
    "        # Parse the root directory and extract the basenames of individual LTE and NR frames.\n",
    "        files = glob.glob(os.path.join(root, \"*.png\"))\n",
    "        self.frames = [os.path.basename(frame).split(\".\")[0] for frame in files]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[Image, Image]:\n",
    "        \"\"\"Return the image-mask pair at idx.\"\"\"\n",
    "        basename = self.frames[idx]\n",
    "\n",
    "        image_file = os.path.join(self.root, f\"{basename}.png\")\n",
    "        target_file = os.path.join(self.root, f\"{basename}.hdf\")\n",
    "\n",
    "        image = Image.open(image_file)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        mask_data = gdal.Open(target_file)\n",
    "        mask_band = mask_data.GetRasterBand(1)\n",
    "        mask = (Image.fromarray(mask_band.ReadAsArray())).convert(mode=\"L\")\n",
    "        if self.target_transform is not None:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice our `SpectrumSensing` class accepts two functions/transforms: `transform`, which is applied to the spectrogram, \n",
    "and `target_transform`, which is applied to the mask.\n",
    "\n",
    "Both the spectrograms and masks are 256 x 256 pixel images. However, the spectrograms are three channeled, while the masks are single-channeled. This is because the spectrograms are full RGB images, whereas the masks are ternary-valued images, where each pixel takes one of three discrete values:\n",
    "- `0`: Represents noise.\n",
    "- `127`: Representing NR signal.\n",
    "- `255`: Representing LTE signal.\n",
    "\n",
    "To prepare our spectrograms for training, we will convert them from PIL Images to Tensor objects. As required by our model, the images have to be loaded into the range `[0, 1]` and then normalized using a mean of `[0.485, 0.456, 0.406]` and a standard deviation of `[0.229, 0.224, 0.225]`. To prepare our masks for training, we will convert them to Tensor objects, remove the extraneous channel dimension, and update the pixel values such that `0` represents noise, `1` represents NR signal, and `2` represents LTE signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.getcwd()\n",
    "data_root = os.path.join(project_root, \"SpectrumSensingDataset\", \"TrainingData\")\n",
    "\n",
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "class Squeeze(torch.nn.Module):\n",
    "    def forward(self, target: Tensor):\n",
    "        return torch.squeeze(target)\n",
    "\n",
    "\n",
    "class DivideBy127(torch.nn.Module):\n",
    "    def forward(self, target: Tensor):\n",
    "        return torch.div(target, 127, rounding_mode=\"floor\")\n",
    "\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        PILToTensor(),\n",
    "        ToDtype(torch.float, scale=True),\n",
    "        Normalize(mean=mean, std=std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_transform = Compose(\n",
    "    [PILToTensor(), Squeeze(), DivideBy127(), ToDtype(torch.long)]  # Mapping 0 -> 0, 127 -> 1, and 255 -> 2.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Now let's initialize the dataset, and take a closer look at a random training example and the corresponding mask. Due to our transforms, we expect that the image-mask pair will be returned as Tensor objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpectrumSensing(root=data_root, transform=transform, target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_index = np.random.randint(len(dataset))\n",
    "training_example, corresponding_mask = dataset[random_index]\n",
    "\n",
    "print(f\"The full dataset has {len(dataset)} examples. Loading example at index {random_index}:\")\n",
    "print(f\"Spectrogram: {type(training_example)}, {training_example.dtype}, {training_example.size()}\")\n",
    "print(f\"Mask: {type(corresponding_mask)}, {corresponding_mask.dtype}, {corresponding_mask.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset should contain 1,800 samples: 900 NR frames and 900 LTE frames. \n",
    "\n",
    "To gain further insight, let's write some transforms to undo the previous normalization and prepare this image-mask pair for viewing. And, let's build a custom colormap for the masks, with noise as cyan, NR signal as blue, and LTE signal as purple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=[0.0, 0.0, 0.0], std=[1 / x for x in std]),\n",
    "        Normalize(mean=[-x for x in mean], std=[1.0, 1.0, 1.0]),\n",
    "        ToPILImage(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "inv_target_transform = Compose([ToDtype(dtype=torch.uint8), ToPILImage()])\n",
    "\n",
    "training_example = inv_transform(training_example)\n",
    "corresponding_mask = inv_target_transform(corresponding_mask)\n",
    "\n",
    "values, labels, colors = [0, 1, 2], [\"Noise\", \"NR\", \"LTE\"], [\"cyan\", \"blue\", \"purple\"]\n",
    "mask_cmap = ListedColormap(colors)\n",
    "\n",
    "print(f\"Spectrogram: {training_example}\")\n",
    "print(f\"Mask: {corresponding_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(figsize=[8, 3.5], nrows=1, ncols=2)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "ax1.set_title(\"\\nRandom Spectrogram\", fontsize=title_font_size)\n",
    "ax2.set_title(\"Corresponding Mask\", fontsize=title_font_size)\n",
    "ax1.set_ylabel(\"Time [arb. units]\", fontsize=label_font_size)\n",
    "ax2.set_ylabel(\"Time [arb. units]\", fontsize=label_font_size)\n",
    "ax1.set_xlabel(\"Freq. [arb. units]\", fontsize=label_font_size)\n",
    "ax2.set_xlabel(\"Freq. [arb. units]\", fontsize=label_font_size)\n",
    "\n",
    "spect = ax1.imshow(training_example, vmin=0, vmax=255)\n",
    "fig.colorbar(spect, ax=ax1, fraction=0.045, ticks=[0, 255])\n",
    "\n",
    "mask = ax2.imshow(corresponding_mask, cmap=mask_cmap, vmin=0, vmax=2)\n",
    "mask_cbar = fig.colorbar(mask, ax=ax2, cmap=mask_cmap, fraction=0.045, ticks=[0.33, 1, 1.67])\n",
    "mask_cbar.ax.set_yticklabels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can view different examples from the dataset by rerunning the previous few code cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the relative frequencies of the different class labels. This step is critical for identifying imbalance in our dataset. Please note that the following code block might take a few seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = {label: 0 for label in labels}\n",
    "\n",
    "for _, mask in dataset:\n",
    "    arr = np.asarray(mask)\n",
    "    for i, label in enumerate(labels):\n",
    "        class_counts[label] += np.sum(arr == values[i])\n",
    "\n",
    "normalized_counts = np.array(list(class_counts.values())) / sum(list(class_counts.values()))\n",
    "\n",
    "plt.bar(class_counts.keys(), normalized_counts, tick_label=labels, color=colors)\n",
    "plt.title(\"Distribution of Pixel Counts by Class\", fontsize=title_font_size)\n",
    "plt.xlabel(\"Class\", fontsize=label_font_size)\n",
    "plt.ylabel(\"Counts (Normalized)\", fontsize=label_font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our dataset is mostly noise! A classification dataset like this—with skewed class proportions—is called imbalanced.\n",
    "\n",
    "An imbalanced dataset can result in biased and poorly performing models. Models trained on imbalanced data tends to focus more on the majority classes and may not learn enough about the minority classes. To ensure the development of a fair, accurate, and robust model, we will need to address this class imbalance. \n",
    "\n",
    "But first, let's split the dataset into separate training and validation sets. The training dataset is the portion of the dataset that will be used to train the model, while the validation dataset will be held in reserve and used to evaluate the performance of the trained model. Let's start with a simple 80/20 split, where 80% of the dataset is used for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.80\n",
    "n_train_examples = int(len(dataset) * train_split)\n",
    "n_val_examples = len(dataset) - n_train_examples\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(\n",
    "    dataset=dataset, lengths=[n_train_examples, n_val_examples], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"The training split contains {len(train_set)} examples.\")\n",
    "print(f\"The validation split contains {len(val_set)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, data loaders facilitate easy access to samples, efficiently load and batch data, and offer numerous other features to streamline data preprocessing, management, and integration within the training loop. Let's create data loaders for both the training and validation datasets.\n",
    "\n",
    "In PyTorch, the `DataLoader` class allows us to pass a `batch_size` argument, which controls the number of samples used in each pass through the network. Using a small number of training examples each pass is called mini-batching, and can improve efficiency, stabilize training dynamics, and enable scalable training on large datasets. Choosing an appropriate mini-batch size depends on several factors, including the available memory on your hardware, training efficiency constraints, and generalization requirements. However, as with everything in machine learning, we ultimately rely on empirical testing to determine the optimal configuration that maximizes model performance for each specific task and dataset. In this example, we'll use mini-batches containing 4 samples each, which will easily fit on any CPU/GPU without issue and provide reasonable generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "spects, masks = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch of spectrograms: {type(spects)}, {spects.dtype}, {spects.size()}\")\n",
    "print(f\"Batch of masks: {type(masks)}, {masks.dtype}, {masks.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a batch of spectrograms along with their corresponding masks. Please note that the following plotting code is optimized for small batch sizes and may not render as nicely with larger batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spects(spects: list[Image.Image]) -> None:\n",
    "    fig, axes = plt.subplots(figsize=[batch_size * 2, 3], nrows=1, ncols=batch_size, sharey=True)\n",
    "    fig.text(0.5, 0.75, \"Spectrograms\", fontsize=title_font_size, ha=\"center\")\n",
    "    axes[0].set_ylabel(\"Time [arb. units]\", fontsize=label_font_size)\n",
    "    fig.text(0.5, 0.12, \"Freq. [arb. units]\", fontsize=label_font_size, ha=\"center\")\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        im = ax.imshow(spects[i], vmin=0, vmax=255)\n",
    "\n",
    "    fig.subplots_adjust(right=0.90)\n",
    "    cbar_ax = fig.add_axes(rect=[0.93, 0.24, 0.02, 0.5])\n",
    "    fig.colorbar(im, cax=cbar_ax, ticks=[0, 255])\n",
    "\n",
    "\n",
    "def plot_masks(masks: list[Image.Image], prediction: bool = False) -> None:\n",
    "    fig, axes = plt.subplots(figsize=[batch_size * 2, 3], nrows=1, ncols=batch_size, sharey=True)\n",
    "    if prediction:\n",
    "        fig.text(0.5, 0.75, \"Model Predictions\", fontsize=title_font_size, ha=\"center\")\n",
    "    else:\n",
    "        fig.text(0.5, 0.75, \"Masks\", fontsize=title_font_size, ha=\"center\")\n",
    "    axes[0].set_ylabel(\"Time [arb. units]\", fontsize=label_font_size)\n",
    "    fig.text(0.5, 0.12, \"Freq. [arb. units]\", fontsize=label_font_size, ha=\"center\")\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        im = ax.imshow(masks[i], vmin=0, vmax=2, cmap=mask_cmap)\n",
    "\n",
    "    fig.subplots_adjust(right=0.90)\n",
    "    cbar_ax = fig.add_axes(rect=[0.93, 0.24, 0.02, 0.5])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, ticks=[0.33, 1, 1.66])\n",
    "    cbar.ax.set_yticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spects(spects=[inv_transform(i) for i in spects])\n",
    "plot_masks(masks=[inv_target_transform(i) for i in masks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can view different batches from the dataset by rerunning the previous few code cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this example, we'll use a DeepLabV3 model with a MobileNetV3 large backbones. This model is designed to be lightweight and efficient, making it ideal for edge computing devices and quick proof-of-concept demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3  # We are dealing with three classes: Noise, NR, and LTE.\n",
    "model = deeplabv3_mobilenet_v3_large(num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a loss function. A loss function, also known as a cost or objective function, measures how well a machine learning \n",
    "model's predictions match the actual target values. This quantifies the error between predicted outputs and ground truth labels, providing\n",
    "feedback that guides the model's training process. For classification problems, we commonly use the [Cross-Entropy Loss](https://machinelearningmastery.com/cross-entropy-for-machine-learning/), especially for \n",
    "multi-class classification problems. Let's use the [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) class from PyTorch, which allows us to assign different weights to individual classes during the computation of the loss. \n",
    "\n",
    "We'll use weights inversely proportional to the relative pixel count for each class. That way, we assign lower weights to overrepresented classes, like noise, and larger weights to underrepresented classes, like LTE signal. This reduces the impact of noise and allows the model to prioritize learning from NR and especially LTE samples. Class weighting is not the only way to address data imblance, but it is one of the more straightforward methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_count = statistics.median(list(class_counts.values()))\n",
    "weight = [median_count / class_counts[k] for k in class_counts.keys()]\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor(weight, dtype=torch.float))\n",
    "\n",
    "print(\"Class weights: \", {k: round(weight[i], 2) for i, k in enumerate(class_counts.keys())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will train out model using stochastic gradient descent (SGD). SGD is a variant of the standard [gradient descent](https://builtin.com/data-science/gradient-descent) optimizer where the loss function is computed on mini-batches of data rather than the entire dataset. This helps improve computational efficiency and scalability, particularly for large datasets, by updating model parameters based on the gradients computed on our mini-batches.\n",
    "\n",
    "We'll define the training and validation process of our segmentation model in a [`LightningModule`](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#lightningmodule). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationSGD(L.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        loss_function: nn.Module,\n",
    "        n_classes: int,\n",
    "        learning_rate: float,\n",
    "        momentum: float,\n",
    "        weight_decay: float,\n",
    "        step_size: int,\n",
    "        gamma: float,\n",
    "    ):\n",
    "        \"\"\"Initializes the SegmentationSGD module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.train_accuracy = MulticlassAccuracy(num_classes=self.n_classes)\n",
    "        self.val_accuracy = MulticlassAccuracy(num_classes=self.n_classes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Defines a forward pass through the model.\"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        \"\"\"Defines a single training step.\"\"\"\n",
    "        image, target = batch\n",
    "        preds = self(image)[\"out\"]\n",
    "        loss = self.loss_function(preds, target)\n",
    "        self.train_accuracy(preds, target)\n",
    "        self.log(name=\"train_accuracy\", value=self.train_accuracy, prog_bar=True)\n",
    "        self.log(name=\"train_loss\", value=loss, on_epoch=True, on_step=False, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        \"\"\"Defines a single validation step.\"\"\"\n",
    "        image, target = batch\n",
    "        preds = self(image)[\"out\"]\n",
    "        loss = self.loss_function(preds, target)\n",
    "        self.val_accuracy(preds, target)\n",
    "        self.log(name=\"val_accuracy\", value=self.val_accuracy, prog_bar=True)\n",
    "        self.log(name=\"val_loss\", value=loss, on_epoch=True, on_step=False, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> dict[str, Any]:\n",
    "        \"\"\"Configure the optimizer and learning rate scheduler.\"\"\"\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(), lr=self.learning_rate, momentum=self.momentum, weight_decay=self.weight_decay\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=self.step_size, gamma=self.gamma)\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `SegmentationModelSGD` is initialized with several configuration settings that influence the behavior and performance of the machine learning algorithm or model. These parameters are called hyperparameters, and unlike model parameters, which are learned from the data during training, hyperparameters are set prior to training and influence the learning process.\n",
    "\n",
    "The following hyperparameters are used to configure the optimizer:\n",
    "- **Momentum:** A parameter that accelerates SGD in the relevant direction and dampens oscillations.\n",
    "- **Learning Rate:** The rate at which the model parameters are updated during optimization.\n",
    "- **Weight Decay:** A regularization term added to the loss function to penalize large weights in the model to prevent overfitting\n",
    "\n",
    "By gradually reducing the learning rate over epochs, the scheduler can help improve the convergence and stability of the optimization process\n",
    "We need to provide the following two parameters, which the learning rate scheduler uses to dynamically adjust the learning rate during training:\n",
    "- **Step Size:** The number of epochs after which the learning rate is reduced.\n",
    "- **Gamma:** The factor by which the learning rate is reduced after every step-size epochs.\n",
    "\n",
    "Adjusting these hyperparameters can significantly impact the training process and the final performance of the model, for better or for worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_module = SegmentationSGD(\n",
    "    model=model,\n",
    "    loss_function=loss_function,\n",
    "    n_classes=n_classes,\n",
    "    learning_rate=0.02,  # Represents the initial learning rate.\n",
    "    momentum=0.9,\n",
    "    weight_decay=1.0e-04,\n",
    "    step_size=10,\n",
    "    gamma=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model, weighted loss function, and Lightning Module, we are prepared to train our model. If available, we will leverage GPU acceleration. Otherwise, the training process will default to using the CPU. Please be patient; model training time may vary depending on the current hardware configuration and could take a few minutes.\n",
    "\n",
    "The number of epochs determines how many times the entire dataset will be used to train the model. For this specific model and dataset, 10 epochs should be more than sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Training model on GPU.\")\n",
    "    trainer = L.Trainer(accelerator=\"gpu\", max_epochs=n_epochs, logger=True)\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"Training model on CPU.\")\n",
    "    trainer = L.Trainer(max_epochs=n_epochs, logger=True)\n",
    "    device = \"cpu\"\n",
    "\n",
    "trainer.fit(model=segmentation_module, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this example locally, you can refer to the `metrics.csv` file located in the `lightning_logs` directory for more information regarding training and validation loss and accuracy across training epochs. Please remember to close the `metrics.csv` file before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "\n",
    "Having trained our model, the next step is to evaluate its performance. To accomplish this, we'll use a suite of standard machine learning metrics. But first, let's take a look at a random batch of predictions and true labels.\n",
    "\n",
    "Because the model returns the unnormalized probabilities corresponding to the predictions of each class. We need to use `argmax()` to get the maximum prediction of each class. The result is a ternary-valued image for each example in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "spects, masks = next(iter(train_loader))\n",
    "spects = spects.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = (model(spects)[\"out\"]).argmax(1)\n",
    "\n",
    "print(\"Predictions:\", preds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spects(spects=[inv_transform(i.cpu()) for i in spects])\n",
    "plot_masks(masks=[inv_target_transform(i) for i in masks])\n",
    "plot_masks(masks=preds.cpu(), prediction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good! But to get a more ojective sense, let's turn to the metrics. Let's start with model accuracy, calculated as the ratio of correctly predicted pixels to the total number of pixels.\n",
    "\n",
    "**Note:** You can view different predictions by rerunning the previous few code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = trainer.validate(model=segmentation_module, dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy can give us a quick sense of the model's overall performance. However, accuracy alone doesn't tell the whole story. In fact, because of the imbalance in our dataset, a reasonably high accuracy could be achieved by always predicting  noise. The simple accuracy provided above is an unweighted mean of the accuracies over each class.\n",
    "\n",
    "To gain a better understanding of our model's ability to predict specific classes, let's take a look at the confusion matrix, which provides a more comprehensive overview of model capability. The diagonal elements represent the correct predictions and off-diagonal elements indicate prediction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "confusion_matrix = MulticlassConfusionMatrix(num_classes=n_classes, normalize=\"true\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for spect, mask in val_loader:\n",
    "        spect, mask = spect.to(device), mask.to(device)\n",
    "        pred = (model(spect)[\"out\"]).argmax(dim=1)\n",
    "        confusion_matrix.update(pred, mask)\n",
    "\n",
    "confusion_matrix = np.round(confusion_matrix.compute().cpu().numpy(), decimals=2)\n",
    "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=labels).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's generate a more comprehensive report, complete with the following metrics:\n",
    "\n",
    "- **Recall:** The recall (sensitivity) measures the ability of the model to identify the relevant pixels. A higher recall indicates that the model is better at identifying signal.\n",
    "\n",
    "- **Precision:** The precision assesses the accuracy of positive predictions. A higher precision indicates that when the model predicts signal, it is more likely to be correct.\n",
    "\n",
    "- **F1 Score:** The F1 score combines both recall and precision into a single value, providing a more balanced measure of the model's performance. A higher F1 indicates a model with both good precision and recall (fewer false positives and false negatives overall).\n",
    "\n",
    "- **Intersection over Union (IoU):** The IoU, commonly called Jaccard's Index, quantifies the overlap between the predicted bounding box or segmented region and the ground truth. A higher IoU value indicates a better alignment between the predicted and actual regions, reflecting a more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_report(dataloader: DataLoader) -> None:\n",
    "    \"\"\"Compute accuracy, recall, precision, F1 score, and IoU (Intersection over Union),\n",
    "    and print a report containing these metrics.\"\"\"\n",
    "    metrics = [\n",
    "        MulticlassAccuracy(num_classes=n_classes, average=None),\n",
    "        MulticlassRecall(num_classes=n_classes, average=None),\n",
    "        MulticlassPrecision(num_classes=n_classes, average=None),\n",
    "        MulticlassF1Score(num_classes=n_classes, average=None),\n",
    "        MulticlassJaccardIndex(num_classes=n_classes, average=None),\n",
    "    ]\n",
    "    metrics = [m.to(device) for m in metrics]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for spect, mask in dataloader:\n",
    "            spect, mask = spect.to(device), mask.to(device)\n",
    "            pred = (model(spect)[\"out\"]).argmax(dim=1)\n",
    "            for m in metrics:\n",
    "                m.update(pred, mask)\n",
    "\n",
    "    metrics = [m.compute().cpu().numpy() for m in metrics]\n",
    "    metrics = [np.append(m, statistics.mean(m)) for m in metrics]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Class\": np.append(np.asarray(labels), \"Mean\"),\n",
    "            \"Accuracy\": metrics[0],\n",
    "            \"Recall\": metrics[1],\n",
    "            \"Precision\": metrics[2],\n",
    "            \"F1 Score\": metrics[3],\n",
    "            \"Jaccard Index\": metrics[4],\n",
    "        }\n",
    "    )\n",
    "    print(\n",
    "        tabulate(\n",
    "            df, headers=\"keys\", tablefmt=\"grid\", showindex=False, numalign=\"center\", stralign=\"center\", floatfmt=\".2f\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics_report(dataloader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Data\n",
    "\n",
    "In machine leaning, generalization refers to the ability of a trained model to perform well on new data that it hasn't been trained on. As an easy way to test the generalization of our model, let's test on the combined frames with both LTE and NR signal. As a reminder, such frames were excluded from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_data = os.path.join(data_root, \"LTE_NR\")\n",
    "challenge_dataset = SpectrumSensing(root=challenge_data, transform=transform, target_transform=target_transform)\n",
    "\n",
    "challenge_loader = DataLoader(challenge_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spects, masks = next(iter(challenge_loader))\n",
    "spects = spects.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = (model(spects)[\"out\"]).argmax(1)\n",
    "\n",
    "print(\"Predictions:\", preds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spects(spects=[inv_transform(i.cpu()) for i in spects])\n",
    "plot_masks(masks=[inv_target_transform(i) for i in masks])\n",
    "plot_masks(masks=preds.cpu(), prediction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can view different examples by rerunning the previous few code cells.\n",
    "\n",
    "Now, let's evaluate the same metrics as we did above in the [Model Validation](#Model-Validation) section, but now for the challenge dataset. Given that these combined frames represent a more challenging problem, we anticipate the model's capabilities to be somewhat diminished, yet we still anticipate reasonable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics_report(dataloader=challenge_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions & Next Steps\n",
    "\n",
    "In this example, we used PyTorch and PyTorch Lightning to train DeepLabV3 models to identify and differentiate between 5G NR and 4G LTE signals within wideband spectrograms, showcasing one of the ways we can leverage machine learning to identify things in the wireless spectrum. This involved data analysis and preprocessing, choosing a loss function and optimizer, model training, model performance validation, and finally testing the model's generalization on combined frames containing both NR and LTE signals, \n",
    "\n",
    "The capability to differentiate and recognize various signals finds direct applications in spectrum sensing, which is fundamental to autonomous spectrum management, and brings us one step closer to more holistic cognitive radio solutions! 📡🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope this example was informative. Here are some next steps you can take to further explore and expand upon what you've learned:\n",
    "\n",
    "- **Experiment with the Hyperparameters:** Adjust the values of hyperparameters such as the number of training epochs, batch size, and learning rate, and observe how these configurations influence model training, performance, and generalization capabilities. After gaining insights through manual hyperparameter tuning, explore automated approaches using tools like [Ray Tune](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) or [Optuna](https://optuna.org/).\n",
    "\n",
    "- **Experiment with DeepLabV3's ResNet Models:** DeepLabV3 also provides models with ResNet-50 and ResNet-101 backbones. These ResNet models are deeper and more complex, and generally offers better model performance than MobileNetV3, which is designed to be lightweight and efficient. Because all DeepLabV3 models implement the same interface, no code changes are required. However, some hyperparameter tuning and/or a larger dataset may be required to train these models effectively. These models have already been imported for your convenience.\n",
    "\n",
    "- **Explore Alternative Solutions to Class Imbalance:** In this example, we addressed class imbalance in our dataset using a weighted cross-entropy loss function. Research and implement alternative strategies or loss functions designed to address imbalance in image datasets.\n",
    "\n",
    "- **Integrate Combined Frames:** In this example, we trained exclusively on the individual NR and LTE frames. Try integrating the combined frames that contain both the NR and LTE signals into the training process, and evaluate the effect on model performance.\n",
    "\n",
    "- **Test your Model on Captured Radio Data:** If you have radio hardware available, consider testing your model on real recordings of live radio data. Check out this article from MathWorks for more information on how to capture NR and LTE signals: [Capture and Label NR and LTE Signals for AI Training](https://www.mathworks.com/help/wireless-testbench/ug/capture-and-label-nr-and-lte-signals-for-ai-training.html).\n",
    "\n",
    "- **Explore RIA Core on GitHub:** At Qoherent, we're building [Radio Intelligence Applications](https://qoherent.ai/radiointelligenceapps-project/) (RIA) to drive the creation of intelligent radios. Check out [RIA Core](https://github.com/qoherent/ria)—the free and open-source foundation of RIA—and consider contributing to the project. ⭐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
