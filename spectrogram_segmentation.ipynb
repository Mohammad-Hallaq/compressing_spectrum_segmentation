{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram Segmentation\n",
    "\n",
    "In this example, we use PyTorch and PyTorch Lightning to train deep learning models to differentiate between \n",
    "5G NR and 4G LTE signals within wideband spectrograms.s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "This example is divided into the following sections:\n",
    "\n",
    "**[Background Information](#background):** Contextualize the problem and introduce the machine learning frameworks and tools used in this example.\n",
    "\n",
    "**[Set-up](#set-up):** Install the necessary libraries.\n",
    "\n",
    "**[Data Preprocessing](data-preprocessing):** Load and analyze the Spectrum Sensing 5G dataset.\n",
    "\n",
    "**[Model Training](#model-training):** Train a machine learning model on the dataset.\n",
    "\n",
    "**[Model Verification](#model-verification):** Asses model performance using a suite common machine learning metrics\n",
    "\n",
    "**[Challenge Data](#challange-data):** Challange the model with combined frames with both LTE and NR signal.\n",
    "\n",
    "**[Conclusions & Next Step](#conclusion-&-next-steps):** Interpret results, summarize learnings, and suggest further steps to extend this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "5G NR (New Radio) and 4G LTE (Long-Term Evolution) are both cellular network technologies, but they represent \n",
    "different generations of mobile network standards. Being able to distinguish between the two, holds significant \n",
    "applications in [spectrum sensing](https://iopscience.iop.org/article/10.1088/1742-6596/2261/1/012016#:~:text=In%20cognitive%20radio%2C%20spectrum%20sensing,user%20can%20use%20the%20spectrum.) and serves as a foundational example showcasing the near-term feasibility of \n",
    "[intelligent radio](https://www.qoherent.ai/intelligentradio/) technology.\n",
    "\n",
    "A spectrogram, which depicts the frequency spectrum of a signal over time, is just an image. Consequently, we \n",
    "apply state-of-the-art [semantic segmentation](https://www.ibm.com/topics/semantic-segmentation) techniques from \n",
    "the field of computer vision to the problem of spectrogram anlysis. Our task is simply to assign one of the \n",
    "following labels to each pixel: 'LTE', 'NR', 'Noise'. ('Noise' refers to the absence of signal, representing \n",
    "a vacant or empty spectrum, also known as whitespace.)\n",
    "\n",
    "The machine learning models utilized in this example are DeepLabV3 models featuring ResNet-50 or MobileNet-V3 backbones. The DeepLabv3 framework was originally introduced by Chen _et al._ in their 2017 paper titled '[Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587)'. For an accessible introduction to the DeepLabV3 framework, please check out Isaac Berrios' article: [DeepLabv3: Biulding Clocks for Robust Segmentation Models](https://medium.com/@itberrios6/deeplabv3-c0c8c93d25a4).\n",
    "\n",
    "The dataset used in this example is the Spectrum Sensing 5G dataset, provided by MathWorks. This dataset contains 900 LTE frames, 900 NR frames, and 900 combined frames with both LTE and NR signal. In this example, we train exclusively on the individual LTE and NR examples, excluding the combined frames.\n",
    "\n",
    "To ensure comparability with results obtained using MathWorks' AI-based network, we use the the hyperparameter configuration from MathWorks' spectrum sensing example: [Deep Learning Toolbox](https://www.mathworks.com/products/deep-learning.html): [Spectrum Sensing with Deep Learning to Identify 5G and LTE Signals](https://www.mathworks.com/help/comm/ug/spectrum-sensing-with-deep-learning-to-identify-5g-and-lte-signals.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))  # Project root\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from PIL import Image\n",
    "from osgeo import gdal\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import v2\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "from torchmetrics import JaccardIndex as jac_ind\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each frame in the dataset, there are three corresponding files:\n",
    "\n",
    "`.png`: The spectrogram image.\n",
    "\n",
    "`.hdf`: The target mask.\n",
    "\n",
    "`.mat`: Frame metadata, details such as the signal sample rate and the number of DFT points used for spectrogram computation. None of this metadata is necessary for this example, so we can safely ignore these files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our dataset is specifically tailored for computer vision tasks, let's extend the [VisionDataset](https://pytorch.org/vision/main/generated/torchvision.datasets.VisionDataset.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrumSensing(VisionDataset):\n",
    "    \"\"\"MathWorks' Spectrum Sensing 5G dataset.\"\"\"\n",
    "\n",
    "    # Mapping of pixel labels for different waveforms.\n",
    "    pixel_labels = {\"Noise\": 0, \"NR\": 127, \"LTE\": 255}  # noise is red, green is NR (5G), blue is LTE (4G)\n",
    "\n",
    "    def __init__(self, root, transforms=None, transform=None, target_transform=None):\n",
    "        \"\"\"Initialize the Spectrum Sensing 5G dataset with the root directory of the dataset and\n",
    "        any necessary functions/transforms.\n",
    "        \"\"\"\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        self.root = root\n",
    "\n",
    "        # Parse the dataset to extract the file names of individual LTE and NR frames.\n",
    "        files = glob.glob(os.path.join(root, \"*.png\"))\n",
    "        self.frames = [os.path.basename(frame).split(\".\")[0] for frame in files]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[Image, Image]:\n",
    "        \"\"\"Get the image-mask pair at idx.\"\"\"\n",
    "        basename = self.frames[idx]\n",
    "\n",
    "        image_file = os.path.join(self.root, f\"{basename}.png\")\n",
    "        target_file = os.path.join(self.root, f\"{basename}.hdf\")\n",
    "\n",
    "        print(\"image file:\\t\", image_file)\n",
    "        print(\"pixel label file:\\t\", target_file)\n",
    "\n",
    "        image = Image.open(image_file)\n",
    "\n",
    "        mask_data = gdal.Open(target_file)\n",
    "        mask_band = mask_data.GetRasterBand(1)\n",
    "        mask = (Image.fromarray(mask_band.ReadAsArray())).convert(mode=\"P\", palette=0)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, mask = self.transforms(image, mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.getcwd()\n",
    "path_to_training_data = os.path.join(project_root, \"SpectrumSensingDataset\", \"TrainingData\")\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]  # Required by our model\n",
    "std = [0.229, 0.224, 0.225]  # Required by our model\n",
    "\n",
    "# transforms:\n",
    "# 1. Convert image to tensor, and rescale to [0.0, 1.0]\n",
    "# 2. Change the dtype of the pixel numbers to float\n",
    "# 3. Normalize using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]\n",
    "\n",
    "transforms = v2.Compose([v2.PILToTensor(), v2.ToDtype(dtype=torch.float32), v2.Normalize(mean=mean, std=std)])\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = SpectrumSensing(root=path_to_training_data, transforms=transforms)\n",
    "print(f\"The full dataset has {len(dataset)} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a random example\n",
    "random_index = np.random.randint(len(dataset))\n",
    "random_image, random_mask = dataset[random_index]\n",
    "print(f\"Loading image at index {random_index}:\\n{dataset[random_index]}\")\n",
    "\n",
    "# TODO: Show the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "labels = [0, 1, 2]  # 0->Noise, 1->NR, 2->LTE\n",
    "freq = {label: 0 for label in labels}  # Frequency of pixels corresponding to each label\n",
    "for _, y in dataset:\n",
    "    for label in labels:\n",
    "        out = np.sum(y.numpy() == label)\n",
    "        freq[label] += out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_heights = np.array(list(freq.values())) / sum(list(freq.values()))\n",
    "\n",
    "plt.bar(freq.keys(), normalized_heights, tick_label=[\"Noise\", \"NR\", \"LTE\"], color=[\"red\", \"green\", \"orange\"])\n",
    "plt.xlabel(\"signal names\", color=\"blue\")\n",
    "plt.ylabel(\"Normalized\\nFrequency of Pixels\", color=\"blue\")\n",
    "plt.title(\"Distribution of Pixels Across Signals\", color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: Our dataset seems to be imbalanced. We will need to take this fact into our account when training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training and Validation Set\n",
    "\n",
    "The data is split into a training and validation set according to: 80% is training set and 20% is validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_size = len(dataset)\n",
    "train_split = 0.80\n",
    "train_size = round(len(dataset) * train_split)\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size], generator=torch.Generator().manual_seed(0)\n",
    ")\n",
    "\n",
    "print(\"Full dataset has {} instances\".format(full_size))\n",
    "print(\"Training subset has {} instances\".format(len(train_subset)))\n",
    "print(\"Validation subset has {} instances\".format(len(val_subset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Deep Neural Network\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps needed to train our model:\n",
    "\n",
    "- Create loaders (training and validation)\n",
    "- Download the model and use Pytorch-Lightning to train the model with  the specified hyperparameter values and a loss function (hyperparameter values are taken from the MATLAB project).\n",
    "\n",
    "These steps are implemented in great detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Training and Validation Dataloaders_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 4\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH, shuffle=True, num_workers=5)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH, shuffle=False, num_workers=2)\n",
    "\n",
    "img_batch, target_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Shape of image batch tensor: \", img_batch.shape, img_batch.dtype)\n",
    "print(\"Shape of mask batch tensor: \", target_batch.shape, target_batch.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Download the Model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model, either ResNet-50 or MobileNetV3\n",
    "NUM_CLASSES = 3\n",
    "# model = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(num_classes=NUM_CLASSES)\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet50(num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Loss function and Hyperparameter values_\n",
    "\n",
    "- To deal with imbalanced dataset, we create function `balance_weights` that outputs the class weights that will be passed to the loss function. Balancing weights is just one tehcnique we can use to address the imbalance in the training data.\n",
    "\n",
    "- The Loss function is a crossentropy loss function.\n",
    "\n",
    "- Optimizer: SGD with \n",
    "     - `momentum = 0.9`\n",
    "     - `lr = 0.2`\n",
    "     - `weight_decay = 1.0e-04` (l2-regularization)\n",
    "- Learning Rate Scheduler:\n",
    "  - `Stepwise` with `stepsize = 5` and `gamma = 0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {\"Noise\": 0, \"NR\": 1, \"LTE\": 2}\n",
    "\n",
    "\n",
    "def balance_weights(dataset: VisionDataset):\n",
    "    \"\"\"\n",
    "    This function outputs the weights for each class by computing\n",
    "     the relative frequency for each class and then taking the inverse of\n",
    "     these relative frequencies.\n",
    "    \"\"\"\n",
    "    pixel_count = {k: (0, 0) for k in class_labels.keys()}\n",
    "\n",
    "    # Count pixels of each class label\n",
    "    for i in range(len(dataset)):\n",
    "        _, mask = dataset[i]\n",
    "        mask = mask.numpy()\n",
    "        pixel_count_i = {k: (mask == v).sum() for k, v in class_labels.items()}\n",
    "\n",
    "        assert (\n",
    "            sum(pixel_count_i.values()) == mask.size\n",
    "        ), \"Sum of each pixel label count does not equal total pixels in mask\"\n",
    "\n",
    "        pixel_count = {\n",
    "            k: (\n",
    "                (pixel_count[k][0] + pixel_count_i[k], pixel_count[k][1] + mask.size)\n",
    "                if (pixel_count_i[k] != 0)\n",
    "                else (pixel_count[k][0] + pixel_count_i[k], pixel_count[k][1])\n",
    "            )\n",
    "            for k in pixel_count_i.keys()\n",
    "        }\n",
    "    # print(\"Pixel count of each label: \\n{}\".format(pixel_count))\n",
    "\n",
    "    # make nx2 array of count values, col 0 for label pixel count, col 1 for image pixel count\n",
    "    rows = len(pixel_count.values())\n",
    "    cols = len(next(iter(pixel_count.values())))\n",
    "    pixel_count_arr = np.zeros((rows, cols), dtype=int)\n",
    "    for i, v in enumerate(pixel_count.values()):\n",
    "        for j in range(len(v)):\n",
    "            pixel_count_arr[i][j] = v[j]\n",
    "    # print(pixel_count_arr)\n",
    "\n",
    "    # calculate frequency of each label\n",
    "    class_freq = pixel_count_arr[:, 0] / pixel_count_arr[:, 1]\n",
    "    # calculate weight of each label\n",
    "    class_weights = np.median(class_freq) / class_freq\n",
    "    print(class_weights, class_weights.dtype)\n",
    "\n",
    "    return torch.as_tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the class weights\n",
    "class_weights = balance_weights(train_subset)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Model will train on: {device}\")\n",
    "class_weights = class_weights.to(device)\n",
    "print(type(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Introduce the loss function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "# Pass the class_weights to the loss function\n",
    "\n",
    "\n",
    "def criterion(results, target, weight=class_weights):\n",
    "    # The output of the model is a dictionary. So, results is a dictionary\n",
    "    losses = {}\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "    for i, result in results.items():\n",
    "        losses[i] = loss_fn(result, target)\n",
    "\n",
    "    return losses[\"out\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Integrate the downloaded model with Pytorch-Lightning environment \n",
    " including the desirable optimizer and learning rate scheduler and the initial learning rate_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hyperparamters needed for SGD (stochastic gradient descent)\n",
    "initial_lr = 0.02  # Represents the initial learning rate, which is the step size used to update the model's parameters\n",
    "# during each iteration of SGD\n",
    "momentum = 0.9  # Accelerates SGD in the relevant direction and dampens oscillations\n",
    "weight_decay = 1.0e-04  # A regularization term added to the loss function to penalize large weights in the model\n",
    "\n",
    "# Initialize hyperparameters needed for learning scheduler\n",
    "step_size = 10  # Represents the number of training epochs after which the learning rate will be adjusted\n",
    "gamma = 0.1  # The multiplicative factor by which the learning rate is reduced after every step_size epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModelSGD(pl.LightningModule):\n",
    "    def __init__(self, model, num_classes, lr, momentum, weight_decay, step_size, gamma, optimizer_name=\"SGD\"):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "        self.train_acc = MulticlassAccuracy(num_classes=self.num_classes)\n",
    "        self.val_acc = MulticlassAccuracy(num_classes=self.num_classes)\n",
    "        self.lr = lr\n",
    "        self.optimizer = getattr(torch.optim, optimizer_name)\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, target = batch\n",
    "        preds = self(image)  # Dictionary\n",
    "        loss = criterion(preds, target)\n",
    "        preds = preds[\"out\"].argmax(dim=1)  # Our prediction\n",
    "        self.train_acc.update(preds, target)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log(\"train_accuracy\", self.train_acc.compute(), prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image, target = batch\n",
    "        preds = self(image)\n",
    "        loss = criterion(preds, target)\n",
    "        preds = preds[\"out\"].argmax(dim=1)\n",
    "        self.val_acc.update(preds, target)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_accuracy\", self.val_acc.compute(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer(\n",
    "            self.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=self.step_size, gamma=self.gamma)\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm_sgd_model = SegmentationModelSGD(\n",
    "    model,\n",
    "    lr=initial_lr,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay,\n",
    "    step_size=step_size,\n",
    "    gamma=gamma,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "# NUM_EPOCHS = 3\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=NUM_EPOCHS, logger=True)\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=NUM_EPOCHS, logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(segm_sgd_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with signals from the validation set\n",
    " \n",
    "We employ the following metrics to evaluate the performance of our model:\n",
    "\n",
    " - Confusiion Matrix: Provides a comprehensive overview of the model's ability, highlighting areas of success and misclassification, with diagonal elements representing correct predictions and off-diagonal elements indicating errors.\n",
    " \n",
    " - Accuracy: The ratio of correctly predicted instances to the total number of instances.\n",
    " \n",
    " - Recall: The recall (Sensitivity) measures the ability of a model to identify all relevant instances.\n",
    "\n",
    " - Precision: The precision assesses the accuracy of positive predictions.\n",
    "\n",
    " - F1: The F1 Score combines both recall and precision, providing a measure of the harmonic mean of precision and recall.\n",
    "\n",
    " - Intersection over Union (IoU) and Histogram of IoU: The IoU quantifies the overlap between the predicted bounding box or segmented region and the ground truth bounding box or annotated region from a dataset. A higher IoU value indicates a better alignment between the predicted and actual regions, reflecting a more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Confusion Matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(\n",
    "    model, val_loader: DataLoader, num_classes: int, device: str, normalize: str | None = \"true\"\n",
    ") -> MulticlassConfusionMatrix:\n",
    "    \"\"\"\n",
    "    Computes and displays the confusion matrix for a given PyTorch model using a validation DataLoader.\n",
    "\n",
    "    :param model: The PyTorch model to evaluate.\n",
    "    :param val_loader: The validation DataLoader.\n",
    "    :param num_classes: The number of classes in the classification problem.\n",
    "\n",
    "    :param device: The device on which to perform the evaluation (e.g., 'cuda' for GPU or 'cpu').\n",
    "\n",
    "    :param normalize: Type of normalization applied to the confusion matrix. Options: {'none', 'true', 'pred', 'all'}.\n",
    "\n",
    "    :return: The confusion matrix.\n",
    "    \"\"\"\n",
    "    conf_matrix = (MulticlassConfusionMatrix(num_classes=num_classes, normalize=normalize)).to(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = (model(x)[\"out\"]).argmax(dim=1)\n",
    "            # conf_matrix.update(pred.to('cpu'),y.to('cpu'))\n",
    "            conf_matrix.update(pred, y)\n",
    "\n",
    "    # Round the entries in the confusiion matrix to two decimals\n",
    "    conf_matrix_rounded = torch.round(conf_matrix.compute(), decimals=2)\n",
    "\n",
    "    # ConfusionnMatrixDisplay expects for the input an array and not a tensor\n",
    "    fig, ax = plt.subplots(1, figsize=(3, 3))\n",
    "    displ = ConfusionMatrixDisplay(np.array(conf_matrix_rounded.to(\"cpu\")))\n",
    "    displ.plot(ax=ax, colorbar=False)\n",
    "    ax.set_xlabel(\"True label\", color=\"blue\")\n",
    "    ax.set_ylabel(\"Predicted label\", color=\"blue\")\n",
    "    ax.set_title(\"Confusion Matrix\", color=\"blue\")\n",
    "\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(segm_sgd_model, val_loader, NUM_CLASSES, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Validation Acccuracy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm_sgd_model.eval()\n",
    "scores = trainer.validate(segm_sgd_model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Histogram of Intersection over Union (IoU) Scores per Image_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(model, loader, device):\n",
    "    iou_scores = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = (model(x)[\"out\"]).argmax(dim=1)\n",
    "            jaccard = jac_ind(task=\"multiclass\", num_classes=3).to(device)\n",
    "            score = jaccard(pred.to(device), y)\n",
    "            iou_scores.append(score.item())\n",
    "\n",
    "    plt.hist(iou_scores, color=\"green\", histtype=\"bar\")\n",
    "    plt.xlabel(\"IoU\", color=\"blue\")\n",
    "    plt.ylabel(\"Number of Masks\", color=\"blue\")\n",
    "    plt.title(\"Mean IoU\", color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_val_loader = DataLoader(val_subset, batch_size=1, shuffle=False)  # Changed the batch_size\n",
    "plot_hist(segm_sgd_model, new_val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Recall, Precision, F1 Score, IOU_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(model, loader, metric_name, num_classes, device):\n",
    "    metric = getattr(torchmetrics.classification, metric_name)\n",
    "    metric_per_class = metric(num_classes=num_classes, average=\"none\").to(device)\n",
    "    average_metric = metric(num_classes=num_classes, average=\"macro\").to(device)\n",
    "    weighted_metric = metric(num_classes=num_classes, average=\"weighted\").to(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = (model(x)[\"out\"]).argmax(dim=1)\n",
    "            pred = pred.to(device)\n",
    "            metric_per_class.update(pred, y)\n",
    "            average_metric.update(pred, y)\n",
    "            weighted_metric.update(pred, y)\n",
    "\n",
    "    value_per_class = metric_per_class.compute()\n",
    "    average_value = average_metric.compute()\n",
    "    weighted_value = weighted_metric.compute()\n",
    "    return value_per_class, average_value.unsqueeze(0), weighted_value.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"MulticlassRecall\", \"MulticlassPrecision\", \"MulticlassF1Score\", \"MulticlassJaccardIndex\"]\n",
    "\n",
    "metric_results = {\n",
    "    metric_name: torch.hstack(compute_metric(segm_sgd_model, val_loader, metric_name, NUM_CLASSES, device))\n",
    "    for metric_name in metric_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_results_cpu = {key: (value.to(\"cpu\")).numpy() for key, value in metric_results.items()}\n",
    "\n",
    "classification_report = pd.DataFrame(metric_results_cpu)\n",
    "\n",
    "index = [\"Noise\", \"NR\", \"LTE\", \"macro avg\", \"weighted avg\"]\n",
    "columns = [\"recall\", \"precision\", \"f1-score\", \"IoU\"]\n",
    "\n",
    "classification_report.columns = columns\n",
    "classification_report.index = index\n",
    "\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Identification in Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_label(mask) -> str:\n",
    "    \"\"\"\n",
    "    :param mask: The mask image containing signal labels.\n",
    "\n",
    "    :return: The signal label based on unique labels in the mask.\n",
    "    \"\"\"\n",
    "    labels = {0: \"Noise\", 1: \"NR\", 2: \"LTE\"}\n",
    "    unique_labels_in_mask = torch.unique(mask)\n",
    "\n",
    "    if len(unique_labels_in_mask) == 2:\n",
    "        key = torch.unique(mask)[1].item()\n",
    "        return labels[key]\n",
    "\n",
    "    elif len(unique_labels_in_mask) == 3:\n",
    "        key_1 = torch.unique(mask)[1].item()\n",
    "        key_2 = torch.unique(mask)[2].item()\n",
    "        return labels[key_1] + \"_\" + labels[key_2]\n",
    "\n",
    "    else:\n",
    "        key = torch.unique(mask)[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram_mask(image, mask: Image, target: Image = None):\n",
    "\n",
    "    if target is not None:\n",
    "        fig, ax = plt.subplots(3, 1, figsize=(4, 10))\n",
    "\n",
    "        ax[0].imshow(torch.permute(image, (1, 2, 0)))\n",
    "        ax[0].set_xlabel(\"Frequency\", fontsize=12, color=\"blue\")\n",
    "        ax[0].set_ylabel(\"Time\", fontsize=12, color=\"blue\")\n",
    "        ax[0].set_title(f\"Received Spectrogram ({signal_label(target)})\", color=\"blue\")\n",
    "\n",
    "        ax[1].set_ylabel(\"Time\", fontsize=12, color=\"blue\")\n",
    "        ax[1].set_xlabel(\"Frequency\", fontsize=12, color=\"blue\")\n",
    "        ax[1].imshow(target)\n",
    "        ax[1].set_title(f\"True Signal Label ({signal_label(target)})\", color=\"blue\")\n",
    "\n",
    "        ax[2].imshow(mask)\n",
    "        # plt.imshow(predicted_image.permute(1,2,0)[:,:,0])\n",
    "        # or equivalnetly\n",
    "        # plt.imshow(pred['out'][0][0].to('cpu').detach())\n",
    "        ax[2].set_xlabel(\"Frequency\", fontsize=12, color=\"blue\")\n",
    "        ax[2].set_ylabel(\"Time\", fontsize=12, color=\"blue\")\n",
    "        ax[2].set_title(f\"Prediction ({signal_label(mask)})\", color=\"blue\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "    else:\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(4, 10))\n",
    "\n",
    "        ax[0].imshow(torch.permute(image, (1, 2, 0)))\n",
    "        ax[0].set_xlabel(\"Frequency\", fontsize=12, color=\"blue\")\n",
    "        ax[0].set_ylabel(\"Time\", fontsize=12, color=\"blue\")\n",
    "        ax[0].set_title(\"Received Spectrogram\", color=\"blue\")\n",
    "\n",
    "        ax[1].imshow(mask)\n",
    "        # plt.imshow(predicted_image.permute(1,2,0)[:,:,0])\n",
    "        # or equivalnetly\n",
    "        # plt.imshow(pred['out'][0][0].to('cpu').detach())\n",
    "        ax[1].set_xlabel(\"Frequency\", fontsize=12, color=\"blue\")\n",
    "        ax[1].set_ylabel(\"Time\", fontsize=12, color=\"blue\")\n",
    "        ax[1].set_title(f\"Prediction ({signal_label(mask)})\", color=\"blue\")\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target = next(iter(val_loader))  # First batch of spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target = image.to(device), target.to(device)\n",
    "segm_sgd_model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_masks = segm_sgd_model(image)[\"out\"]\n",
    "    first_mask_in_batch = predicted_masks[0].argmax(dim=0)\n",
    "\n",
    "first_image_in_batch = image[0]\n",
    "first_target_in_batch = target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram_mask(first_image_in_batch.to(\"cpu\"), first_mask_in_batch.to(\"cpu\"), first_target_in_batch.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model with Combined (NR_LTE) Signals\n",
    "\n",
    "_Note_ : Combined NR_LTE signals were excluded from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the first NR_LTE signal\n",
    "spec_path = os.path.join(PATH_TO_MATLAB5G_TRAINING_DATA, \"LTE_NR\", \"LTE_NR_frame_0.png\")\n",
    "spectrogram = read_image(spec_path)  # Image has both NR and LTE signal\n",
    "spectrogram = spectrogram.to(device)\n",
    "\n",
    "segm_sgd_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = segm_sgd_model((spectrogram.to(torch.float)).unsqueeze(0))[\"out\"]\n",
    "    mask = pred[0].argmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram_mask(spectrogram.to(\"cpu\"), mask.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Conclusion_ \n",
    "\n",
    "We checked the performance of our model using a spectrogram that has both LTE and NR\n",
    "signals (and Noise). However, the prediction made by the model on the same spectrogram depicts only NR signal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spectrogram-segmentation",
   "language": "python",
   "name": "spectrogram-segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
